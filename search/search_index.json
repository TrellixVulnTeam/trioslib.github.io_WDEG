{
    "docs": [
        {
            "location": "/",
            "text": "TRIOSlib documentation\n\n\nTRIOSlib is a library that contains implementations of state-of-the-art methods in Image Operator Learning.\nIt has been used, in its many incarnations, in many research papers over the last 15 years. \nThe current version (2.1) has had many parts rewritten and was redesigned to be flexible and \nto allow users to combine and extend each of its parts. \n\n\nDeveloped in Cython, TRIOSlib is many times faster than equivalent pure Python code, while \nstill being expressive and easy to use.\n\n\nWe do not have yet a research paper specifically on TRIOSlib, so if you use it in a paper, please cite\n\n\nMontagner, I. S.; Hirata, R.; Hirata, N.S.T., \"A Machine Learning Based Method for Staff Removal,\" Pattern Recognition (ICPR), 2014 22nd International Conference on.\n\n\nInstalling TRIOSlib\n\n\nTo install trioslib from \nPyPI\n use \n\n\n\n\n$ pip install trios\n\n\n\n\nTo install from source first clone the repository and then run setup.py install.\n\n\n\n\n$ git clone https://github.com/trioslib/trios.git \n\n\n$ cd trios\n\n\n$ python setup.py install\n\n\n\n\nWe recommend using trios with \nAnaconda Python\n.\n\n\nQuickstart\n\n\nIf you are new to Image Transform Learning we recommend reading our review\nentitled \nImage Operator Learning and Applications\n (\nopen access\n). See the \nUser guide\n for instructions on how to use TRIOSlib and a brief presentation of all concepts needed.",
            "title": "Home"
        },
        {
            "location": "/#trioslib-documentation",
            "text": "TRIOSlib is a library that contains implementations of state-of-the-art methods in Image Operator Learning.\nIt has been used, in its many incarnations, in many research papers over the last 15 years. \nThe current version (2.1) has had many parts rewritten and was redesigned to be flexible and \nto allow users to combine and extend each of its parts.   Developed in Cython, TRIOSlib is many times faster than equivalent pure Python code, while \nstill being expressive and easy to use.  We do not have yet a research paper specifically on TRIOSlib, so if you use it in a paper, please cite  Montagner, I. S.; Hirata, R.; Hirata, N.S.T., \"A Machine Learning Based Method for Staff Removal,\" Pattern Recognition (ICPR), 2014 22nd International Conference on.",
            "title": "TRIOSlib documentation"
        },
        {
            "location": "/#installing-trioslib",
            "text": "To install trioslib from  PyPI  use    $ pip install trios   To install from source first clone the repository and then run setup.py install.   $ git clone https://github.com/trioslib/trios.git   $ cd trios  $ python setup.py install   We recommend using trios with  Anaconda Python .",
            "title": "Installing TRIOSlib"
        },
        {
            "location": "/#quickstart",
            "text": "If you are new to Image Transform Learning we recommend reading our review\nentitled  Image Operator Learning and Applications  ( open access ). See the  User guide  for instructions on how to use TRIOSlib and a brief presentation of all concepts needed.",
            "title": "Quickstart"
        },
        {
            "location": "/user-guide/introduction/",
            "text": "Introduction\n\n\nThis user guide presents a broad view of all concepts needed to use TRIOSlib. All code examples presented can be found at the \ndocs/examples\n folder. \n\n\nImage sets\n\n\nThe image processing task we would like to solve is defined by a set of input-output images. For instance, if the task to be solved is staff removal, the following pair of images coudl be used for training. We would like to estimate an image transform that takes as input the image on the left and produces an output as close as possile to the image on the right. \n\n\n\n    \n\n        \n\n        \n\n    \n\n    \n\n        \n Figure 1 \n: Input-output image pair for staff removal\n\n    \n\n\n\n\n\nThe \ntrios.Imageset\n class represents this set of input-output images and resembles a list of tuples. Each element of \ntrios.Imageset\n contains the path to an input image, the path to the desired output and the path to a binary mask that indicates which pixels should be processed. The following example illustrates the usage of the \ntrios.Imageset\n class to create sets of input-output images. \n\n\nimport trios\nimport trios.shortcuts.persistence as p\n\nif __name__ == '__main__':\n    # Imageset can be defined directly in code, like below\n    images = trios.Imageset([\n        ('input.png', 'output.png', None),\n        ('input2.png', 'output2.png', 'mask.png')\n    ])\n    # Definitions in code can be put in a module and imported like regular\n    # Python code.\n\n\n    # It can also be saved to a gziped file using the persistence\n    p.save_gzip(images, 'imageset.gz')\n\n    # And loaded back\n    images2 = p.load_gzip('imageset.gz')\n    assert images[0] == images2[0]\n    assert images[1] == images2[1]\n    assert len(images) == len(images2)\n\n    # Or saved to a text-only format using the read and write methods.\n    images.write('imageset-text.txt')\n\n    images3 = trios.Imageset.read('imageset-text.txt')\n    assert images[0] == images3[0]\n    assert images[1] == images3[1]\n    assert len(images) == len(images3)\n\n\n\n\nWindows\n\n\nImage operators built with TRIOS are local image transforms that analyse\na small neighborhood window around each point to determine its value in the \noutput image. TRIOS uses 2D numpy arrays of unsigned 8bit ints to store windows. \nA point is inside the window if its value not \n0\n (we typically use \n1\n). Thus, the size (number of points) of a window \nwin\n is \nnp.sum(win != 0)\n. \n\n\nThe follow example illustrates the use of the module \ntrios.shortcuts.window\n to create simple window shapes. More interesting shapes can be built using slice notation. \n\n\nimport numpy as np\nimport trios.shortcuts.window as trios_win\n\nif __name__ == '__main__': \n    # We can use the trios.shortcuts.window module to create windows.abs\n    rect5x5 = trios_win.square(5)\n    # The show method plots a window using matplotlib. Black dots\n    # represent points inside the window.\n    trios_win.show(rect5x5, 'rect5x5.png')\n\n    # We can create rectangular shapes as well\n    rect9x7 = trios_win.rectangle(7, 9)\n    trios_win.show(rect9x7, 'rect9x7.png')\n\n    # Windows are just numpy 2D arrays of unsigned 8bit ints.\n    # We can use array slices to create new shapes.\n    cross = np.zeros((5, 5), np.uint8)\n    cross[2,:] = cross[:, 2] = 1\n    trios_win.show(cross, 'cross5x5.png')\n\n\n\n\n\n    \n\n        \n\n        \n\n        \n\n    \n\n    \n\n        \nSquare \\(5\\times 5\\)\n\n        \nRectangle \\(9\\times 7\\)\n\n        \nCross \\(5\\times 5\\)\n\n    \n\n\n\n\n\nWOperators\n\n\nLocal image transforms (or image operators) estimated by TRIOS are called \nWOperators\n and are represented by the \ntrios.WOperator\n class. In the following pages we will learn how to use instances of this class to transform images and how to estimate new local transformations.",
            "title": "Introduction"
        },
        {
            "location": "/user-guide/introduction/#introduction",
            "text": "This user guide presents a broad view of all concepts needed to use TRIOSlib. All code examples presented can be found at the  docs/examples  folder.",
            "title": "Introduction"
        },
        {
            "location": "/user-guide/introduction/#image-sets",
            "text": "The image processing task we would like to solve is defined by a set of input-output images. For instance, if the task to be solved is staff removal, the following pair of images coudl be used for training. We would like to estimate an image transform that takes as input the image on the left and produces an output as close as possile to the image on the right.   \n     \n         \n         \n     \n     \n          Figure 1  : Input-output image pair for staff removal \n       The  trios.Imageset  class represents this set of input-output images and resembles a list of tuples. Each element of  trios.Imageset  contains the path to an input image, the path to the desired output and the path to a binary mask that indicates which pixels should be processed. The following example illustrates the usage of the  trios.Imageset  class to create sets of input-output images.   import trios\nimport trios.shortcuts.persistence as p\n\nif __name__ == '__main__':\n    # Imageset can be defined directly in code, like below\n    images = trios.Imageset([\n        ('input.png', 'output.png', None),\n        ('input2.png', 'output2.png', 'mask.png')\n    ])\n    # Definitions in code can be put in a module and imported like regular\n    # Python code.\n\n\n    # It can also be saved to a gziped file using the persistence\n    p.save_gzip(images, 'imageset.gz')\n\n    # And loaded back\n    images2 = p.load_gzip('imageset.gz')\n    assert images[0] == images2[0]\n    assert images[1] == images2[1]\n    assert len(images) == len(images2)\n\n    # Or saved to a text-only format using the read and write methods.\n    images.write('imageset-text.txt')\n\n    images3 = trios.Imageset.read('imageset-text.txt')\n    assert images[0] == images3[0]\n    assert images[1] == images3[1]\n    assert len(images) == len(images3)",
            "title": "Image sets"
        },
        {
            "location": "/user-guide/introduction/#windows",
            "text": "Image operators built with TRIOS are local image transforms that analyse\na small neighborhood window around each point to determine its value in the \noutput image. TRIOS uses 2D numpy arrays of unsigned 8bit ints to store windows. \nA point is inside the window if its value not  0  (we typically use  1 ). Thus, the size (number of points) of a window  win  is  np.sum(win != 0) .   The follow example illustrates the use of the module  trios.shortcuts.window  to create simple window shapes. More interesting shapes can be built using slice notation.   import numpy as np\nimport trios.shortcuts.window as trios_win\n\nif __name__ == '__main__': \n    # We can use the trios.shortcuts.window module to create windows.abs\n    rect5x5 = trios_win.square(5)\n    # The show method plots a window using matplotlib. Black dots\n    # represent points inside the window.\n    trios_win.show(rect5x5, 'rect5x5.png')\n\n    # We can create rectangular shapes as well\n    rect9x7 = trios_win.rectangle(7, 9)\n    trios_win.show(rect9x7, 'rect9x7.png')\n\n    # Windows are just numpy 2D arrays of unsigned 8bit ints.\n    # We can use array slices to create new shapes.\n    cross = np.zeros((5, 5), np.uint8)\n    cross[2,:] = cross[:, 2] = 1\n    trios_win.show(cross, 'cross5x5.png')  \n     \n         \n         \n         \n     \n     \n         Square \\(5\\times 5\\) \n         Rectangle \\(9\\times 7\\) \n         Cross \\(5\\times 5\\)",
            "title": "Windows"
        },
        {
            "location": "/user-guide/introduction/#woperators",
            "text": "Local image transforms (or image operators) estimated by TRIOS are called  WOperators  and are represented by the  trios.WOperator  class. In the following pages we will learn how to use instances of this class to transform images and how to estimate new local transformations.",
            "title": "WOperators"
        },
        {
            "location": "/user-guide/using_trained_operators/",
            "text": "Using trained operators\n\n\nThe first step to work with TRIOSlib is to use a trained image transform to process images. We will be working with the \njung\n dataset, which consists in detecting the letter \ns\n in a digitized document image. \n\n\n\n    \n\n        \n\n        \n\n    \n\n    \n\n        \n Figure 1 \n: Input-output image pair for *jung* dataset\n\n    \n\n\n\n\n\nThe dataset can be downloaded in \nthis link\n or using the script \ndocs/examples/download_jung_images.py\n. \n\n\nTrained image transforms are saved to the disk using the function \nsave_gzip\n from the \ntrios.shortcuts.persistence\n module and loaded using \nload_gzip\n.  As the methods' names imply, they are stored in compressed mode to save disk space and to decrease loading times. To load images we recommend the \nload_image\n function (also from \ntrios.shortcuts.persistence\n), as it loads images in the byte format expected by TRIOSlib. Images are represented by numpy arrays. \n\n\nTrained operators are instances of the \ntrios.WOperator\n class. We are interested in the \nWOperator.apply\n and \nWOperator.eval\n methods. \n\n\nThe \nWOperator.apply(img, mask)\n method takes two images as arguments. The first argument, \nimg\n, is the input image to be transformed and the second argument, \nmask\n, is an optional mask image. To process the whole image just pass \nNone\n in the second argument. \n\n\nThe \nWOperator.eval(testset)\n method evaluates the performance of an image transform on a given \ntrios.Imageset\n. For each triple in the Image set it produces a processed version of the input image and (ii) compares it with the expected output. Then the ratio of pixels with incorrect pixel values is computed and returned. Smaller values mean that the processed output resembles more closely the expected one.\n\n\nThe example below illustrates the use of these methods to detect the letter \ns\n in images from the \njung\n dataset. To use it, download \nthis operator\n save it as \ntrained-jung.op.gz\n in the same folder as the code below. It is also necessary to download the \njung\n dataset and uncompress and name it \njung-images\n (the script \ndownload_jung_images.py\n already does this).\n\n\nimport trios.shortcuts.persistence as p\nimport trios\nimport numpy as np\n\nif __name__ == '__main__':\n    jung_op = p.load_gzip('trained-jung.op.gz')\n\n    input13 = p.load_image('jung-images/jung-15b.png')\n    # The jung operator uses the input image as mask. Thus, it erases \n    # all pixels that do not belong to an s.\n    result = jung_op.apply(input13, input13) \n    p.save_image(result, 'jung13-output.png')\n\n    testset = trios.Imageset.read('jung-images/test.set')\n    print('Error:', jung_op.eval(testset))\n\n\n\n\n\nIt should produce an error of about \n0.005\n, which is a respectable value for this dataset.\n\n\nAcademic users can determine which papers to cite (and read!) by calling \nWOperator.cite_me()\n. Each technique implemented in TRIOSlib defines a \nbibtex\n entry that is included when \ncite_me()\nis called. The example below illustrates its use. \n\n\nimport trios.shortcuts.persistence as p\nimport trios\nimport numpy as np\n\nif __name__ == '__main__':\n    jung_op = p.load_gzip('trained-jung.op.gz')\n    print(jung_op.cite_me())\n\n\n\n\n\n\n@inproceedings{montagner2016image,\n  title={Image operator learning and applications},\n  author={Montagner, Igor S and Hirata, Nina ST and Hirata, Roberto},\n  booktitle={Graphics, Patterns and Images Tutorials (SIBGRAPI-T), SIBGRAPI Conference on},\n  pages={38--50},\n  year={2016},\n  organization={IEEE}\n}\n\n@inproceedings{montagner2016kernel,\n  title={Kernel Approximations for W-operator learning},\n  author={Montagner, Igor S and Hirata, Roberto and Hirata, Nina ST and Canu, St{'e}phane},\n  booktitle={Graphics, Patterns and Images (SIBGRAPI), 2016 29th SIBGRAPI Conference on},\n  pages={386--393},\n  year={2016},\n  organization={IEEE}\n}\n\n@article{scikit-learn,\n title={Scikit-learn: Machine Learning in {P}ython},\n author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n journal={Journal of Machine Learning Research},\n volume={12},\n pages={2825--2830},\n year={2011}\n}",
            "title": "Using trained operators"
        },
        {
            "location": "/user-guide/using_trained_operators/#using-trained-operators",
            "text": "The first step to work with TRIOSlib is to use a trained image transform to process images. We will be working with the  jung  dataset, which consists in detecting the letter  s  in a digitized document image.   \n     \n         \n         \n     \n     \n          Figure 1  : Input-output image pair for *jung* dataset \n       The dataset can be downloaded in  this link  or using the script  docs/examples/download_jung_images.py .   Trained image transforms are saved to the disk using the function  save_gzip  from the  trios.shortcuts.persistence  module and loaded using  load_gzip .  As the methods' names imply, they are stored in compressed mode to save disk space and to decrease loading times. To load images we recommend the  load_image  function (also from  trios.shortcuts.persistence ), as it loads images in the byte format expected by TRIOSlib. Images are represented by numpy arrays.   Trained operators are instances of the  trios.WOperator  class. We are interested in the  WOperator.apply  and  WOperator.eval  methods.   The  WOperator.apply(img, mask)  method takes two images as arguments. The first argument,  img , is the input image to be transformed and the second argument,  mask , is an optional mask image. To process the whole image just pass  None  in the second argument.   The  WOperator.eval(testset)  method evaluates the performance of an image transform on a given  trios.Imageset . For each triple in the Image set it produces a processed version of the input image and (ii) compares it with the expected output. Then the ratio of pixels with incorrect pixel values is computed and returned. Smaller values mean that the processed output resembles more closely the expected one.  The example below illustrates the use of these methods to detect the letter  s  in images from the  jung  dataset. To use it, download  this operator  save it as  trained-jung.op.gz  in the same folder as the code below. It is also necessary to download the  jung  dataset and uncompress and name it  jung-images  (the script  download_jung_images.py  already does this).  import trios.shortcuts.persistence as p\nimport trios\nimport numpy as np\n\nif __name__ == '__main__':\n    jung_op = p.load_gzip('trained-jung.op.gz')\n\n    input13 = p.load_image('jung-images/jung-15b.png')\n    # The jung operator uses the input image as mask. Thus, it erases \n    # all pixels that do not belong to an s.\n    result = jung_op.apply(input13, input13) \n    p.save_image(result, 'jung13-output.png')\n\n    testset = trios.Imageset.read('jung-images/test.set')\n    print('Error:', jung_op.eval(testset))  It should produce an error of about  0.005 , which is a respectable value for this dataset.  Academic users can determine which papers to cite (and read!) by calling  WOperator.cite_me() . Each technique implemented in TRIOSlib defines a  bibtex  entry that is included when  cite_me() is called. The example below illustrates its use.   import trios.shortcuts.persistence as p\nimport trios\nimport numpy as np\n\nif __name__ == '__main__':\n    jung_op = p.load_gzip('trained-jung.op.gz')\n    print(jung_op.cite_me())  \n@inproceedings{montagner2016image,\n  title={Image operator learning and applications},\n  author={Montagner, Igor S and Hirata, Nina ST and Hirata, Roberto},\n  booktitle={Graphics, Patterns and Images Tutorials (SIBGRAPI-T), SIBGRAPI Conference on},\n  pages={38--50},\n  year={2016},\n  organization={IEEE}\n}\n\n@inproceedings{montagner2016kernel,\n  title={Kernel Approximations for W-operator learning},\n  author={Montagner, Igor S and Hirata, Roberto and Hirata, Nina ST and Canu, St{'e}phane},\n  booktitle={Graphics, Patterns and Images (SIBGRAPI), 2016 29th SIBGRAPI Conference on},\n  pages={386--393},\n  year={2016},\n  organization={IEEE}\n}\n\n@article{scikit-learn,\n title={Scikit-learn: Machine Learning in {P}ython},\n author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n journal={Journal of Machine Learning Research},\n volume={12},\n pages={2825--2830},\n year={2011}\n}",
            "title": "Using trained operators"
        },
        {
            "location": "/user-guide/training_new_operators/",
            "text": "Training new operators\n\n\nImage transforms in TRIOSlib require three basic parameters: a neighborhood \nwindow $W$, a \nFeature Extractor\n and a \nClassifier\n. Window creation routines\nwere explained in the \nintroduction\n, so in this page we focus\non the other two parameters. In this example we use a small \\(9\\times 7\\) \nrectangular window (shown below).\n\n\n\n    \n\n\n\n\nA \nFeature Extractor\n transforms a 2D image patch \\(X\\) observed using a \nwindow as the one above into a convenient vectorial representation \n\\(\\varphi(X)\\). The most basic method for feature extraction is to use raw \npixel values as features. Thus, in this case it results in a feature vector with \n\\(63\\) features, one for each pixel in our \\(9\\times 7\\) window (in raster order). \nThis simple method is implemented in the \n\ntrios.feature_extractors.RAWFeatureExtractor\n class. \n\n\nA \nClassifier\n in TRIOS determines the output value \\(\\hat y\\) of a pixel in \nthe output image using the vectorial representation \\(\\varphi(x)\\) computed by \nthe previous step (Feature Extraction). In this example we use Decision Trees, \nas they do not require parameter determination and achieve reasonable \nperformance in many problems. We use scikit-learn's implementation \n(\nsklearn.tree.DecisionTreeClassifier\n) via the wrapper class \n\ntrios.classifiers.SKClassifier\n. \n\n\nImages are processed by first applying the feature extractor to a pixel in the \ninput and then using the trained classifier to determine the output value. This \nis done for all pixels in the input image. \n\n\nA local image transform is represented by the \ntrios.WOperator\n class, which \ncombines specific feature extraction and classification methods to effectively\nprocess images. This class implements the learning process for image transforms \nby appplying the selected feature method to all pixels of the input images of\na \ntrios.Imageset\n and coupling their vectorial representations with the desired\noutput values to train a classifier. \n\n\nTo illustrate the creation of a local image transform using TRIOS we will \nprocess the \njung\n dataset again. \njung\n serves a role in image transform \nlearning similar to MNIST in Deep Learning: an easy problem that can be used\nto quickly test new ideas. \n\n\nThe code below creates an image transform using a \\(9\\times 7\\) window, raw\npixel values as features and a decision tree as classifier. The method \n\nWOperator.train\n takes as input a \ntrios.Imageset\n instance and executes the \nlearning procedure. After saving the image transform can be saved using \n\np.save_gzip(op, file_name)\n it can be loaded again using the instructions from\nthe \nlast section\n.\n\n\n# file docs/examples/creating_image_transforms.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.feature_extractors import RAWFeatureExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\nimport trios.shortcuts.window as trios_win\n\nif __name__ == '__main__':\n    training_set = trios.Imageset.read('jung-images/training.set')\n    test = trios.Imageset.read('jung-images/test.set')\n\n    win = trios_win.rectangle(9, 7)\n    classifier = SKClassifier(DecisionTreeClassifier())\n    fext = RAWFeatureExtractor(win)\n    op = trios.WOperator(win, classifier, fext)\n\n    op.train(training_set)\n    p.save_gzip(op, 'basic-jung.op.gz')\n\n    print('Error: ', op.eval(test))\n\n\n\n\n\nError: 0.010420612911861719\n\n\n\n\nThis example produces as error of about \\(1\\%\\), double the value of the \ntrained transform used in the \nlast section\n. \nEven in this simple problem using advanced techniques can significantly \nincrease performance. The \nAdvanced Methods\n page\nprovides an overview of the implemented methods and when to use them. Each \nmethod is also documented separately and includes an example of use.",
            "title": "Training new operators"
        },
        {
            "location": "/user-guide/training_new_operators/#training-new-operators",
            "text": "Image transforms in TRIOSlib require three basic parameters: a neighborhood \nwindow $W$, a  Feature Extractor  and a  Classifier . Window creation routines\nwere explained in the  introduction , so in this page we focus\non the other two parameters. In this example we use a small \\(9\\times 7\\) \nrectangular window (shown below).  \n       A  Feature Extractor  transforms a 2D image patch \\(X\\) observed using a \nwindow as the one above into a convenient vectorial representation \n\\(\\varphi(X)\\). The most basic method for feature extraction is to use raw \npixel values as features. Thus, in this case it results in a feature vector with \n\\(63\\) features, one for each pixel in our \\(9\\times 7\\) window (in raster order). \nThis simple method is implemented in the  trios.feature_extractors.RAWFeatureExtractor  class.   A  Classifier  in TRIOS determines the output value \\(\\hat y\\) of a pixel in \nthe output image using the vectorial representation \\(\\varphi(x)\\) computed by \nthe previous step (Feature Extraction). In this example we use Decision Trees, \nas they do not require parameter determination and achieve reasonable \nperformance in many problems. We use scikit-learn's implementation \n( sklearn.tree.DecisionTreeClassifier ) via the wrapper class  trios.classifiers.SKClassifier .   Images are processed by first applying the feature extractor to a pixel in the \ninput and then using the trained classifier to determine the output value. This \nis done for all pixels in the input image.   A local image transform is represented by the  trios.WOperator  class, which \ncombines specific feature extraction and classification methods to effectively\nprocess images. This class implements the learning process for image transforms \nby appplying the selected feature method to all pixels of the input images of\na  trios.Imageset  and coupling their vectorial representations with the desired\noutput values to train a classifier.   To illustrate the creation of a local image transform using TRIOS we will \nprocess the  jung  dataset again.  jung  serves a role in image transform \nlearning similar to MNIST in Deep Learning: an easy problem that can be used\nto quickly test new ideas.   The code below creates an image transform using a \\(9\\times 7\\) window, raw\npixel values as features and a decision tree as classifier. The method  WOperator.train  takes as input a  trios.Imageset  instance and executes the \nlearning procedure. After saving the image transform can be saved using  p.save_gzip(op, file_name)  it can be loaded again using the instructions from\nthe  last section .  # file docs/examples/creating_image_transforms.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.feature_extractors import RAWFeatureExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\nimport trios.shortcuts.window as trios_win\n\nif __name__ == '__main__':\n    training_set = trios.Imageset.read('jung-images/training.set')\n    test = trios.Imageset.read('jung-images/test.set')\n\n    win = trios_win.rectangle(9, 7)\n    classifier = SKClassifier(DecisionTreeClassifier())\n    fext = RAWFeatureExtractor(win)\n    op = trios.WOperator(win, classifier, fext)\n\n    op.train(training_set)\n    p.save_gzip(op, 'basic-jung.op.gz')\n\n    print('Error: ', op.eval(test))  \nError: 0.010420612911861719  This example produces as error of about \\(1\\%\\), double the value of the \ntrained transform used in the  last section . \nEven in this simple problem using advanced techniques can significantly \nincrease performance. The  Advanced Methods  page\nprovides an overview of the implemented methods and when to use them. Each \nmethod is also documented separately and includes an example of use.",
            "title": "Training new operators"
        },
        {
            "location": "/user-guide/extending_trios/",
            "text": "Extending TRIOSlib\n\n\nTRIOSlib supports user created Feature Extractors and Classifiers. Although many \nmethods are available in the \ntrios.contrib\n package, user created methods can \nbe located anywhere as long as they are importable. To support saving and \nloading in different machines it is recommended to wrap custom methods in \nredistributable python packages. \n\n\nCreating new feature extractors\n\n\nTo create a new feature extractor one need to subclass the \ntrios.feature_extractor.FeatureExtractor\n\nclass and override the following methods:\n\n\nclass NewFeatureExtractor(FeatureExtractor):\n    def __init__(self, win): \n    # initialize new method with the desired numpy data type!\n    super(self, NewFeatureExtractor).__init__(win, dtype=np.float)\n\n    def __len__(self):\n    pass # this method returns the number of features extracted.\n\n    def extract(self, img, i, j, pat):\n    pass # records the pattern at position img[i,j] in pat.\n\n    def write_state(self, obj_dict):\n    pass # returns a dict with fields to save to a file.\n\n    def set_state(self, obj_dict):\n    pass # uses obj_dict to deserialize the object.\n\n\n\n\nSee \nthe code of \nRAWFeatureExtractor\n for a reference implementation. \n\n\nPreprocessing using Scikit-learn\n\n\nMany feature extractors can be seen as a pre-processing of the data. If we are \nalready using \nSKClassifier\n to wrap scikit-learn models, we can leverage the \navailable pre-processing in the \n\nsklearn.preprocessing\n \npackage by using a \n\nsklearn.pipeline.Pipeline\n \nas the classifier. See the \nscikit-learn \ndocs\n for more \ndetails. \n\n\nCreating new classifiers\n\n\nIf the classifiers already in \nTRIOSlib\n do not fit your needs, you can \nimplement new ones by subclassing \n\ntrios.classifiers.base_classifier.Classifier\n. Aside from the constructor, \nit is only necessary to implement four methods.\n\n\nclass NewClassifier(trios.classifiers.base_classifier.Classifier):\n    def train(self, dataset, kw):\n        pass\n\n    def partial_train(self, X, y, kw):\n        \"Executes one training iteration\"\n        pass\n\n    def apply(self, fvector):\n        \"Applies to a single element\"\n        pass\n\n    def apply_batch(self, fmatrix):\n        '''\n        Applies in batch to many elements stored as rows in a matrix. A naive\n        version using a for and calling apply is already implemented by default\n        '''\n        pass\n\n\n\n\n\nSee \ntrios.classifiers.SKClassifier\n.\nfor a simple example of the usage of this API.",
            "title": "Extending TRIOSlib"
        },
        {
            "location": "/user-guide/extending_trios/#extending-trioslib",
            "text": "TRIOSlib supports user created Feature Extractors and Classifiers. Although many \nmethods are available in the  trios.contrib  package, user created methods can \nbe located anywhere as long as they are importable. To support saving and \nloading in different machines it is recommended to wrap custom methods in \nredistributable python packages.",
            "title": "Extending TRIOSlib"
        },
        {
            "location": "/user-guide/extending_trios/#creating-new-feature-extractors",
            "text": "To create a new feature extractor one need to subclass the  trios.feature_extractor.FeatureExtractor \nclass and override the following methods:  class NewFeatureExtractor(FeatureExtractor):\n    def __init__(self, win): \n    # initialize new method with the desired numpy data type!\n    super(self, NewFeatureExtractor).__init__(win, dtype=np.float)\n\n    def __len__(self):\n    pass # this method returns the number of features extracted.\n\n    def extract(self, img, i, j, pat):\n    pass # records the pattern at position img[i,j] in pat.\n\n    def write_state(self, obj_dict):\n    pass # returns a dict with fields to save to a file.\n\n    def set_state(self, obj_dict):\n    pass # uses obj_dict to deserialize the object.  See  the code of  RAWFeatureExtractor  for a reference implementation.",
            "title": "Creating new feature extractors"
        },
        {
            "location": "/user-guide/extending_trios/#preprocessing-using-scikit-learn",
            "text": "Many feature extractors can be seen as a pre-processing of the data. If we are \nalready using  SKClassifier  to wrap scikit-learn models, we can leverage the \navailable pre-processing in the  sklearn.preprocessing  \npackage by using a  sklearn.pipeline.Pipeline  \nas the classifier. See the  scikit-learn \ndocs  for more \ndetails.",
            "title": "Preprocessing using Scikit-learn"
        },
        {
            "location": "/user-guide/extending_trios/#creating-new-classifiers",
            "text": "If the classifiers already in  TRIOSlib  do not fit your needs, you can \nimplement new ones by subclassing  trios.classifiers.base_classifier.Classifier . Aside from the constructor, \nit is only necessary to implement four methods.  class NewClassifier(trios.classifiers.base_classifier.Classifier):\n    def train(self, dataset, kw):\n        pass\n\n    def partial_train(self, X, y, kw):\n        \"Executes one training iteration\"\n        pass\n\n    def apply(self, fvector):\n        \"Applies to a single element\"\n        pass\n\n    def apply_batch(self, fmatrix):\n        '''\n        Applies in batch to many elements stored as rows in a matrix. A naive\n        version using a for and calling apply is already implemented by default\n        '''\n        pass  See  trios.classifiers.SKClassifier .\nfor a simple example of the usage of this API.",
            "title": "Creating new classifiers"
        },
        {
            "location": "/user-guide/performance-evaluation/",
            "text": "Performance Evaluation\n\n\nTRIOSlib offers some methods to measure the performance of trained image \ntransforms. In order to compute more reliable performance measures we recommend \nreserving some input-output pairs exclusively for this task in a \ntest set\n. \nMeasuring performance in the input-output pairs used during training is known to \nproduce optimistic values that do not correspond to performance in unknown \nimages. \n\n\n\n\nIn all the examples in this page we set \ntrios.show_eval_progress=False\n to \nhide the progress messages in the evaluation code. We also use the trained \nimage transform from \nUsing trained transforms\n and\nthe \njung\n dataset from \nIntroduction\n.\n\n\n\n\nThe simplest way of measuring performance is using the \nWOperator.eval\n method. \nIt receives as input a \ntrios.Imageset\n and returns the pixel error of the image \ntransform. That is, it computes the proportion of pixels that were assigned an \nincorrect gray-level. \n\n\nThe keyword argument \nbinary=True\n computes binary evaluation measures (True \nPositives, True Negatives, False Positives, False Negatives) and returns them in \nthat order. \n\n\nWOperator.eval\n also supports computing accuracy independently for each image. \nJust pass the keyword argument \nper_image=True\n to obtain a list containing the \nperformance measures for each image. See the code below for an example of use of \nthese measures. \n\n\n# file docs/examples/using_woperator_eval.py\nimport trios.shortcuts.persistence as p\nimport trios.shortcuts.evaluation as ev\nimport trios\n\ntrios.show_eval_progress = False\n\nif __name__ == '__main__':\n    operator = p.load_gzip('trained-jung.op.gz')\n    testset = trios.Imageset.read('jung-images/test.set')\n\n    print('Error:', operator.eval(testset))\n    print('Binary:', operator.eval(testset, binary=True))\n    print('Error per image:', operator.eval(testset, per_image=True))\n\n\n\n\n\nError: 0.005500750102286675\nBinary: (11721, 185163, 563, 526)\nError per image: (0.005500750102286675, [(118, 19767), (95, 21930), (115, 18197), (72, 18931), (116, 20463), (67, 19107), (154, 19646), (54, 20293), (164, 20033), (134, 19606)])\n\n\n\n\nIn the code above we called eval several times with different parameters and \neach time it applied the image transform to the same test images, which is \nnecessary since \nWOperator.eval\n does not save the results. Besides the obvious \nwaste of time (specially for large test sets), visually inspecting the result \nimages is a very good way of getting insight on what the image transform is \ndoing and how to improve its performance. \n\n\nA better way of evaluating performance is by using the functions in the \n\ntrios.shortcuts.evaluation\n module. In all examples we import this module as \n\nev\n, so we will refer to functions in this namespace using the \nev.\n prefix. \n\n\nWe can use the \nev.apply_batch(op, testset, result_folder)\n function to apply an \nimage transform to all images from a testset and save them in the specified \nfolder. Then, we can call \nev.compare_folders(testset, result_folder, window)\n to \ncompute the same performance measures of \nWOperator.eval\n. Do not forget to\npass \noperator.window\n to \nev.compare_folder\n! Since the estimated image transforms\nare local we do not evaluate when the neighborhood selected falls off the image.\n\n\nSee the code below for a simple example. \n\n\n# file docs/examples/evaluation_functions.py\nimport trios\nimport trios.shortcuts.persistence as p\nimport trios.shortcuts.evaluation as ev\n\ntrios.show_eval_progress = False\n\nif __name__ == '__main__':\n    operator = p.load_gzip('trained-jung.op.gz')\n    testset = trios.Imageset.read('jung-images/test.set')\n\n    ev.apply_batch(operator, testset, 'jung-result')\n    print('Error:', ev.compare_folders(testset, 'jung-result', operator.window))\n    print('Binary:', ev.compare_folders(testset, 'jung-result', operator.window, binary=True))\n    print('Error per image', ev.compare_folders(testset, 'jung-result', operator.window, per_image=True))\n\n\n\n\n\nError: 0.00550075010229\nBinary: [ 11721 185163    563    526]\nError per image (0.0055007501022866752, [(118, 19767), (95, 21930), (115, 18197), (72, 18931), (116, 20463), (67, 19107), (154, 19646), (54, 20293), (164, 20033), (134, 19606)])\n\n\n\n\nFinally, we can compute Recall, Specificity, Precision, Negative Preditive Value\nand F\n1\n measure by calling \nev.binary_evaluation(TP, TN, FP, FN)\n, where\n\nTP, TN, FP, FN\n were obtained by calling \nWOperator.eval\n or \nev.compare_folders\n\nwith \nbinary=True\n. The example below prints a performance report based on\nthese measures for problems with binary output images. \n\n\n# file docs/examples/performance_report.py\nimport trios.shortcuts.persistence as p\nimport trios.shortcuts.evaluation as ev\nimport trios \n\ntrios.show_eval_progress = False\n\nif __name__ == '__main__':\n    operator = p.load_gzip('trained-jung.op.gz')\n    testset = trios.Imageset.read('jung-images/test.set')\n\n    mes = ev.compare_folders(testset, 'jung-result', binary=True)\n    acc, recall, precision, specificity, neg_pred, F1 = ev.binary_measures(*mes)\n\n    print('''\nAccuracy: %f\nRecall: %f\nPrecision: %f\nSpecificity: %f\nNPV: %f\nF1: %f'''%(acc, recall, precision, specificity, neg_pred, F1))\n\n\n\n\n\nAccuracy: 0.994369\nRecall: 0.954945\nPrecision: 0.954168\nSpecificity: 0.996972\nNPV: 0.997025\nF1: 0.954557",
            "title": "Performance evaluation"
        },
        {
            "location": "/user-guide/performance-evaluation/#performance-evaluation",
            "text": "TRIOSlib offers some methods to measure the performance of trained image \ntransforms. In order to compute more reliable performance measures we recommend \nreserving some input-output pairs exclusively for this task in a  test set . \nMeasuring performance in the input-output pairs used during training is known to \nproduce optimistic values that do not correspond to performance in unknown \nimages.    In all the examples in this page we set  trios.show_eval_progress=False  to \nhide the progress messages in the evaluation code. We also use the trained \nimage transform from  Using trained transforms  and\nthe  jung  dataset from  Introduction .   The simplest way of measuring performance is using the  WOperator.eval  method. \nIt receives as input a  trios.Imageset  and returns the pixel error of the image \ntransform. That is, it computes the proportion of pixels that were assigned an \nincorrect gray-level.   The keyword argument  binary=True  computes binary evaluation measures (True \nPositives, True Negatives, False Positives, False Negatives) and returns them in \nthat order.   WOperator.eval  also supports computing accuracy independently for each image. \nJust pass the keyword argument  per_image=True  to obtain a list containing the \nperformance measures for each image. See the code below for an example of use of \nthese measures.   # file docs/examples/using_woperator_eval.py\nimport trios.shortcuts.persistence as p\nimport trios.shortcuts.evaluation as ev\nimport trios\n\ntrios.show_eval_progress = False\n\nif __name__ == '__main__':\n    operator = p.load_gzip('trained-jung.op.gz')\n    testset = trios.Imageset.read('jung-images/test.set')\n\n    print('Error:', operator.eval(testset))\n    print('Binary:', operator.eval(testset, binary=True))\n    print('Error per image:', operator.eval(testset, per_image=True))  \nError: 0.005500750102286675\nBinary: (11721, 185163, 563, 526)\nError per image: (0.005500750102286675, [(118, 19767), (95, 21930), (115, 18197), (72, 18931), (116, 20463), (67, 19107), (154, 19646), (54, 20293), (164, 20033), (134, 19606)])  In the code above we called eval several times with different parameters and \neach time it applied the image transform to the same test images, which is \nnecessary since  WOperator.eval  does not save the results. Besides the obvious \nwaste of time (specially for large test sets), visually inspecting the result \nimages is a very good way of getting insight on what the image transform is \ndoing and how to improve its performance.   A better way of evaluating performance is by using the functions in the  trios.shortcuts.evaluation  module. In all examples we import this module as  ev , so we will refer to functions in this namespace using the  ev.  prefix.   We can use the  ev.apply_batch(op, testset, result_folder)  function to apply an \nimage transform to all images from a testset and save them in the specified \nfolder. Then, we can call  ev.compare_folders(testset, result_folder, window)  to \ncompute the same performance measures of  WOperator.eval . Do not forget to\npass  operator.window  to  ev.compare_folder ! Since the estimated image transforms\nare local we do not evaluate when the neighborhood selected falls off the image.  See the code below for a simple example.   # file docs/examples/evaluation_functions.py\nimport trios\nimport trios.shortcuts.persistence as p\nimport trios.shortcuts.evaluation as ev\n\ntrios.show_eval_progress = False\n\nif __name__ == '__main__':\n    operator = p.load_gzip('trained-jung.op.gz')\n    testset = trios.Imageset.read('jung-images/test.set')\n\n    ev.apply_batch(operator, testset, 'jung-result')\n    print('Error:', ev.compare_folders(testset, 'jung-result', operator.window))\n    print('Binary:', ev.compare_folders(testset, 'jung-result', operator.window, binary=True))\n    print('Error per image', ev.compare_folders(testset, 'jung-result', operator.window, per_image=True))  \nError: 0.00550075010229\nBinary: [ 11721 185163    563    526]\nError per image (0.0055007501022866752, [(118, 19767), (95, 21930), (115, 18197), (72, 18931), (116, 20463), (67, 19107), (154, 19646), (54, 20293), (164, 20033), (134, 19606)])  Finally, we can compute Recall, Specificity, Precision, Negative Preditive Value\nand F 1  measure by calling  ev.binary_evaluation(TP, TN, FP, FN) , where TP, TN, FP, FN  were obtained by calling  WOperator.eval  or  ev.compare_folders \nwith  binary=True . The example below prints a performance report based on\nthese measures for problems with binary output images.   # file docs/examples/performance_report.py\nimport trios.shortcuts.persistence as p\nimport trios.shortcuts.evaluation as ev\nimport trios \n\ntrios.show_eval_progress = False\n\nif __name__ == '__main__':\n    operator = p.load_gzip('trained-jung.op.gz')\n    testset = trios.Imageset.read('jung-images/test.set')\n\n    mes = ev.compare_folders(testset, 'jung-result', binary=True)\n    acc, recall, precision, specificity, neg_pred, F1 = ev.binary_measures(*mes)\n\n    print('''\nAccuracy: %f\nRecall: %f\nPrecision: %f\nSpecificity: %f\nNPV: %f\nF1: %f'''%(acc, recall, precision, specificity, neg_pred, F1))  \nAccuracy: 0.994369\nRecall: 0.954945\nPrecision: 0.954168\nSpecificity: 0.996972\nNPV: 0.997025\nF1: 0.954557",
            "title": "Performance Evaluation"
        },
        {
            "location": "/user-guide/advanced_methods/",
            "text": "Advanced methods\n\n\nThe \nbasic training procedure\n presented earlier\nmay achieve acceptable performance in easy problems, but it is not sufficient\nwhen dealing with more challenging tasks. TRIOSlib also offers more sofisticated\nmethods to deal with these cases. \n\n\nThe next sections provide a summary of what each method does, a simple code\nexample and a recommendation of when to use it. All methods also provide a link\nto a scientific article describing them in full detail. \n\n\nGray-level operators are usually trained using \nAperture\n. Very \ngood results were obtained when combined with \nKA\n.\n\n\nFor binary images, \nTwo level\n and \nNILC\n have obtained \ntop performance in some problems.",
            "title": "Advanced methods"
        },
        {
            "location": "/user-guide/advanced_methods/#advanced-methods",
            "text": "The  basic training procedure  presented earlier\nmay achieve acceptable performance in easy problems, but it is not sufficient\nwhen dealing with more challenging tasks. TRIOSlib also offers more sofisticated\nmethods to deal with these cases.   The next sections provide a summary of what each method does, a simple code\nexample and a recommendation of when to use it. All methods also provide a link\nto a scientific article describing them in full detail.   Gray-level operators are usually trained using  Aperture . Very \ngood results were obtained when combined with  KA .  For binary images,  Two level  and  NILC  have obtained \ntop performance in some problems.",
            "title": "Advanced methods"
        },
        {
            "location": "/methods/aperture/",
            "text": "Aperture operators\n\n\n\n\nReference: \nHirata Jr, Roberto, et al. \"Aperture filters: theory, application, \nand multiresolution analysis.\" Advances in Nonlinear Signal and Image Processing \n(2006): 15-45.\n \n\n\n\n\nThe image transforms learned in TRIOS are locally defined inside a small \nneighborhood around each pixel. Aperture transforms also put a vertical window \nof size \\(k\\) centered on the gray-level value of the pixel. Given a window \nW\n \nwith dimensions \\(m\\times n\\), Aperture subtracts the center pixel value from \nall points in the window, saturating at a value \\(k\\) and \\(-k\\). \n\n\nAperture is very useful when dealing with gray-level problems where changes in \nbrightness do not affect the output image. The most notable application of \nAperture was in conjunction with \nTwo Level\n and \nNILC\n \nto do retinal vessel segmentation in the \nDRIVE \ndataset\n (as reported in \nthis \nthesis\n, \nwhich obtained accuracies of about \\(94.4\\)% ). \n\n\nDownload the \nDRIVE dataset\n\nand modify the \ndrive_location\n variable to use the code below. It takes about\n2 minutes to run on a regular desktop machine and represents a very simple\nuse case for Aperture. \n\n\n# file docs/examples/methods/aperture.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.feature_extractors import Aperture\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\ndrive_location = '/media/igor/Data1/datasets/drive'\ntraining = trios.Imageset([\n    ('%s/training/images/%2d_training.tif'%(drive_location, i),\n    '%s/training/1st_manual/%2d_manual1.gif'%(drive_location, i),\n    '%s/training/mask/%2d_training_mask.gif'%(drive_location, i))\n    for i in range(21, 41)])\n\ntestset = trios.Imageset([\n    ('%s/test/images/%02d_test.tif'%(drive_location, i),\n    '%s/test/1st_manual/%02d_manual1.gif'%(drive_location, i),\n    '%s/test/mask/%02d_test_mask.gif'%(drive_location, i)) \n    for i in range(1, 21)])\n\nif __name__ == '__main__':    \n    win = np.ones((9, 9), np.uint8)\n    ap = Aperture(win, 10, mul=0.5)\n    op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), ap)\n    print('Training')\n    op.train(training[:2]) # only two images to speed up\n    print('Evaluating')\n    print('Accuracy:', 1 - op.eval(testset[:2])) # only two images to speed up    \n\n\n\n\n\nAccuracy: 0.9081394727942615",
            "title": "Aperture"
        },
        {
            "location": "/methods/aperture/#aperture-operators",
            "text": "Reference:  Hirata Jr, Roberto, et al. \"Aperture filters: theory, application, \nand multiresolution analysis.\" Advances in Nonlinear Signal and Image Processing \n(2006): 15-45.     The image transforms learned in TRIOS are locally defined inside a small \nneighborhood around each pixel. Aperture transforms also put a vertical window \nof size \\(k\\) centered on the gray-level value of the pixel. Given a window  W  \nwith dimensions \\(m\\times n\\), Aperture subtracts the center pixel value from \nall points in the window, saturating at a value \\(k\\) and \\(-k\\).   Aperture is very useful when dealing with gray-level problems where changes in \nbrightness do not affect the output image. The most notable application of \nAperture was in conjunction with  Two Level  and  NILC  \nto do retinal vessel segmentation in the  DRIVE \ndataset  (as reported in  this \nthesis , \nwhich obtained accuracies of about \\(94.4\\)% ).   Download the  DRIVE dataset \nand modify the  drive_location  variable to use the code below. It takes about\n2 minutes to run on a regular desktop machine and represents a very simple\nuse case for Aperture.   # file docs/examples/methods/aperture.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.feature_extractors import Aperture\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\ndrive_location = '/media/igor/Data1/datasets/drive'\ntraining = trios.Imageset([\n    ('%s/training/images/%2d_training.tif'%(drive_location, i),\n    '%s/training/1st_manual/%2d_manual1.gif'%(drive_location, i),\n    '%s/training/mask/%2d_training_mask.gif'%(drive_location, i))\n    for i in range(21, 41)])\n\ntestset = trios.Imageset([\n    ('%s/test/images/%02d_test.tif'%(drive_location, i),\n    '%s/test/1st_manual/%02d_manual1.gif'%(drive_location, i),\n    '%s/test/mask/%02d_test_mask.gif'%(drive_location, i)) \n    for i in range(1, 21)])\n\nif __name__ == '__main__':    \n    win = np.ones((9, 9), np.uint8)\n    ap = Aperture(win, 10, mul=0.5)\n    op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), ap)\n    print('Training')\n    op.train(training[:2]) # only two images to speed up\n    print('Evaluating')\n    print('Accuracy:', 1 - op.eval(testset[:2])) # only two images to speed up      \nAccuracy: 0.9081394727942615",
            "title": "Aperture operators"
        },
        {
            "location": "/methods/two-level/",
            "text": "Two-level operators\n\n\n\n\nReference: \nNina S. T. Hirata, \"Multilevel Training of Binary Morphological \nOperators\" at IEEE Transactions on Pattern Analysis and Machine Intelligence, \n2009\n. \n\n\n\n\nTwo level operators are an approach to combine several image transforms that use\ndifferent (and relatively small) into a single (and better) image transform. It\nworks by building a new pattern using the output of the combined image \ntransforms. The figure below illustrates this process when combining three\nimage transforms. \n\n\n\n\n\n\n\n\nDesigning these combinations of image transforms requires the determination of \nhow many image transforms to combine and how to determine the parameters of each \none. A simple yet effective approach is to take randomly subsets of moderate \nsize of a large neighborhood. The code below exemplifies this approach in the \n\njung\n dataset. Although results are better than the \nbasic \napproach\n, it is still worse than using more \nsofisticated methods. \n\n\nTwo-level is implemented as a feature extractor called \nCombinationPattern\n. It\nreceives operators as arguments and computes the colored feature vector in the\nfigure above. \n\n\n# file docs/examples/methods/two-level.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.feature_extractors import RAWFeatureExtractor, CombinationPattern\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\nimport trios.shortcuts.window as w\n\nif __name__ == '__main__':\n    np.random.seed(10) # set this to select the same window everytime\n    images = trios.Imageset.read('../jung-images/level1.set')\n    images2 = trios.Imageset.read('../jung-images/level2.set')\n    test = trios.Imageset.read('../jung-images/test.set')\n\n    domain = np.ones((9, 7), np.uint8)\n    windows = [w.random_win(domain, 40, True) for i in range(5)]\n    ops = []\n    for i in range(5):\n        op = trios.WOperator(windows[i], SKClassifier(DecisionTreeClassifier()), RAWFeatureExtractor)\n        print('Training...', i)\n        op.train(images)\n        ops.append(op)\n\n    comb = CombinationPattern(*ops)\n    wop2 = trios.WOperator(comb.window, SKClassifier(DecisionTreeClassifier()), comb) \n    print('Training 2nd level')\n    wop2.train(images2)\n\n    print('Error', wop2.eval(test))\n\n\n\n\n\n\nTraining... 0\nTraining... 1\nTraining... 2\nTraining... 3\nTraining... 4\nTraining 2nd level\nError 0.007666954366731778\n\n\n\n\nAn approach that has also shown to be effective is to use basic shapes such as \ncrosses, lines and rectangles in the individual image transforms. \nThis page\n\nshows some windows used when processing music documents. \n\n\nThe \nNILC\n method is an evolution of this random combination method and\nshould achieve much better results.",
            "title": "Two level"
        },
        {
            "location": "/methods/two-level/#two-level-operators",
            "text": "Reference:  Nina S. T. Hirata, \"Multilevel Training of Binary Morphological \nOperators\" at IEEE Transactions on Pattern Analysis and Machine Intelligence, \n2009 .    Two level operators are an approach to combine several image transforms that use\ndifferent (and relatively small) into a single (and better) image transform. It\nworks by building a new pattern using the output of the combined image \ntransforms. The figure below illustrates this process when combining three\nimage transforms.      Designing these combinations of image transforms requires the determination of \nhow many image transforms to combine and how to determine the parameters of each \none. A simple yet effective approach is to take randomly subsets of moderate \nsize of a large neighborhood. The code below exemplifies this approach in the  jung  dataset. Although results are better than the  basic \napproach , it is still worse than using more \nsofisticated methods.   Two-level is implemented as a feature extractor called  CombinationPattern . It\nreceives operators as arguments and computes the colored feature vector in the\nfigure above.   # file docs/examples/methods/two-level.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.feature_extractors import RAWFeatureExtractor, CombinationPattern\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\nimport trios.shortcuts.window as w\n\nif __name__ == '__main__':\n    np.random.seed(10) # set this to select the same window everytime\n    images = trios.Imageset.read('../jung-images/level1.set')\n    images2 = trios.Imageset.read('../jung-images/level2.set')\n    test = trios.Imageset.read('../jung-images/test.set')\n\n    domain = np.ones((9, 7), np.uint8)\n    windows = [w.random_win(domain, 40, True) for i in range(5)]\n    ops = []\n    for i in range(5):\n        op = trios.WOperator(windows[i], SKClassifier(DecisionTreeClassifier()), RAWFeatureExtractor)\n        print('Training...', i)\n        op.train(images)\n        ops.append(op)\n\n    comb = CombinationPattern(*ops)\n    wop2 = trios.WOperator(comb.window, SKClassifier(DecisionTreeClassifier()), comb) \n    print('Training 2nd level')\n    wop2.train(images2)\n\n    print('Error', wop2.eval(test))  \nTraining... 0\nTraining... 1\nTraining... 2\nTraining... 3\nTraining... 4\nTraining 2nd level\nError 0.007666954366731778  An approach that has also shown to be effective is to use basic shapes such as \ncrosses, lines and rectangles in the individual image transforms.  This page \nshows some windows used when processing music documents.   The  NILC  method is an evolution of this random combination method and\nshould achieve much better results.",
            "title": "Two-level operators"
        },
        {
            "location": "/methods/ISI/",
            "text": "ISI - Incremental Splitting of Intervals\n\n\n\n\nHirata, Nina ST, Roberto Hirata Jr, and Junior Barrera. \"Basis computation \nalgorithms.\" Mathematical Morphhology and its Applications to Signal and Image \nProcessing (Proceedings of the 8th International Symposium on Mathematical \nMorphology). 2007. \n\n\n\n\nISI (short for Incremental Splitting of Intervals) is a classifier specifically\ndesigned for image transform learning. It is based on mathematical morphology\noperators and can be decomposed as a combination of simple image transformations.\n\n\nThis implementation has several limitations: (i) only works with binary inputs;\n(ii) has high memory and CPU consumption and (iii) uses a specific encoding\nfor features, so it can not be coupled with other methods in TRIOS.\n\n\nAccording to some simple tests, ISI does achieve performance slightly better\nthan using Decision Trees from scikit-learn. However, ISI is much more expensive\nin terms of CPU and memory when larger (more than 40) sets of features are used.\n\n\nThe code below illustrates how to use ISI to process the \njung\n dataset. ISI \nonly works with \nRAWBitFeatureExtractor\n and has no parameters. The code below \ntakes about a minute to run. The same approach using a Decision Tree takes less \nthan 10 seconds and results in very similar performance. \n\n\nOnly use ISI with small numbers of features. Windows larger than 40 points\ntake forever to run.\n\n\n# file docs/examples/methods/isi.py\nfrom trios.classifiers.isi import ISI\nfrom trios.feature_extractors import RAWBitFeatureExtractor\nimport trios\n\nimport trios.shortcuts.window as trios_win\n\nif __name__ == '__main__':\n    training_set = trios.Imageset.read('../jung-images/training.set')\n    test = trios.Imageset.read('../jung-images/test.set')        \n    win = trios_win.rectangle(7, 7)\n\n    # ISI requires features coded in the bits of an int32\n    op = trios.WOperator(win, ISI, RAWBitFeatureExtractor)\n    op.train(training_set)\n    print('Error: ', op.eval(test))\n\n\n\n\n\nError:  0.013615260395639888",
            "title": "ISI"
        },
        {
            "location": "/methods/ISI/#isi-incremental-splitting-of-intervals",
            "text": "Hirata, Nina ST, Roberto Hirata Jr, and Junior Barrera. \"Basis computation \nalgorithms.\" Mathematical Morphhology and its Applications to Signal and Image \nProcessing (Proceedings of the 8th International Symposium on Mathematical \nMorphology). 2007.    ISI (short for Incremental Splitting of Intervals) is a classifier specifically\ndesigned for image transform learning. It is based on mathematical morphology\noperators and can be decomposed as a combination of simple image transformations.  This implementation has several limitations: (i) only works with binary inputs;\n(ii) has high memory and CPU consumption and (iii) uses a specific encoding\nfor features, so it can not be coupled with other methods in TRIOS.  According to some simple tests, ISI does achieve performance slightly better\nthan using Decision Trees from scikit-learn. However, ISI is much more expensive\nin terms of CPU and memory when larger (more than 40) sets of features are used.  The code below illustrates how to use ISI to process the  jung  dataset. ISI \nonly works with  RAWBitFeatureExtractor  and has no parameters. The code below \ntakes about a minute to run. The same approach using a Decision Tree takes less \nthan 10 seconds and results in very similar performance.   Only use ISI with small numbers of features. Windows larger than 40 points\ntake forever to run.  # file docs/examples/methods/isi.py\nfrom trios.classifiers.isi import ISI\nfrom trios.feature_extractors import RAWBitFeatureExtractor\nimport trios\n\nimport trios.shortcuts.window as trios_win\n\nif __name__ == '__main__':\n    training_set = trios.Imageset.read('../jung-images/training.set')\n    test = trios.Imageset.read('../jung-images/test.set')        \n    win = trios_win.rectangle(7, 7)\n\n    # ISI requires features coded in the bits of an int32\n    op = trios.WOperator(win, ISI, RAWBitFeatureExtractor)\n    op.train(training_set)\n    print('Error: ', op.eval(test))  \nError:  0.013615260395639888",
            "title": "ISI - Incremental Splitting of Intervals"
        },
        {
            "location": "/methods/nilc/",
            "text": "NILC\n\n\n\n\nReference: I. S. Montagner, N. S. T. Hirata, R. Hirata and S. Canu, \"NILC: A \ntwo level learning algorithm with operator selection,\" 2016 IEEE International \nConference on Image Processing (ICIP), Phoenix, AZ, 2016, pp. 1873-1877. \n\n\n\n\nNILC (short for Near Infinite Linear Combination) is a method for automatically \ncreating combinations of image transforms, also known as \ntwo \nlevel\n transforms. \n\n\nNILC receives as input a large neighborhood window \nW\n and a regularization \nparameter \\(\\lambda\\). It works builds a combination of transformations by, \nat each iteration, selecting a random image transform (based on a random subset \nof window \nW\n) and then verifying if it can improve the current combination. In \nthe positive case it includes this new transform (and possibly discards others).\nThis process is repeat when no more improvements are found after \nmax_age\n iterations\nor after a maximum of \nmax_iter\n. The \\(\\lambda\\) parameter controls how many\ntransforms are selected and is typicall larger than \\(1\\). \n\n\nThere are two ways of using NILC. The first way (\non the fly\n) executes the \nmethod iteravely and runs for at most \nmax_iter\n iterations, but possibly less.\nThe second method (\npre-computed\n) first trains all \nmax_iter\n transforms and \nthen executes the NILC algorithm. Although there is large possibility of wasting\nresources training transforms that will not be used, this allows faster iteration\nwhen determining \\(\\lambda\\).\n\n\nOn the fly\n\n\nThe code below illustrates how to use NILC \non the fly\n. This is good if you happen\nto know a very good value for \\(\\lambda\\) or if it can be easily determined somehow.\n\n\n# file docs/examples/methods/nilc1.py\nimport trios\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.classifiers import SKClassifier\nfrom trios.feature_extractors import RAWFeatureExtractor\n\nfrom trios.contrib.nilc.nilc import nilc, plot_progress\nimport trios.shortcuts.window as w\n\nimport numpy as np\n\n# Activate logging so that we can follow NILC progress\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\ndef operator_with_random_window(training_set, domain_window):\n    rnd = np.random.RandomState(12) # set this to select the same windows everytime\n    while True:\n        win = w.random_win(domain_window, 40, rndstate=rnd)\n        op = trios.WOperator(win, \n                             SKClassifier(DecisionTreeClassifier()), \n                             RAWFeatureExtractor)\n        op.train(training_set)\n        yield op\n\nif __name__ == '__main__':\n    np.random.seed(10) \n    images = trios.Imageset.read('../jung-images/level1.set')\n    images2 = trios.Imageset.read('../jung-images/level2.set')\n    test = trios.Imageset.read('../jung-images/test.set')\n\n    domain = np.ones((9, 7), np.uint8)\n\n    op2, progress = nilc(images, images2, operator_with_random_window, domain, lamb=4.2, max_iter=50, max_age=10)\n\n    print('Final error:', op2.eval(test))\n    print('Number of transforms:', len(op2.extractor))\n    plot_progress(progress)\n\n\n\n\n\nFinal error: 0.0043036171599157465\nNumber of transforms: 8\n\n\n\n\nOn the fly NILC receives many arguments. The first two are \ntrios.Imageset\ns to \nbe used during training. \n\n\nThe \noperator_with_random_window\n function can either returning a list of \n\nmax_iter\n operators or it can be a generator that yields new operators. If you \ndo not know what a generator function is follow \nthis \nlink\n for a simple \nexplanation. Or just follow the template above and create a function with an \ninfinite loop and a \nyield\n inside it. \n\n\nThe \ndomain\n argument is a large neighborhood window. All transforms combined\nare restricted to this window. The \nlamb\n, \nmax_iter\n and \nmax_age\n arguments\nwere already explained above. \n\n\nPre-computed\n\n\nThe code below illustrates how to use NILC \nprecomputed\n. This is useful if you\ndo not know which \\(\\lambda\\) to use and want to run the method many times with\ndifferent values.\n\n\n# file docs/examples/methods/nilc2.py\nimport trios\nfrom trios.contrib.nilc.nilc import nilc_precomputed, plot_progress\nimport trios.shortcuts.window as w\n\nimport numpy as np\n\n# Activate logging so that we can follow NILC progress\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\nfrom nilc1 import operator_with_random_window\nfrom trios.feature_extractors import CombinationPattern\nimport itertools\n\nif __name__ == '__main__':\n    np.random.seed(10) \n    images = trios.Imageset.read('../jung-images/level1.set')\n    images2 = trios.Imageset.read('../jung-images/level2.set')\n    test = trios.Imageset.read('../jung-images/test.set')\n\n    domain = np.ones((9, 7), np.uint8)\n\n    # Train all 50 transforms before starting NILC and apply them to images2\n    # This takes the fist 50 elements from the iterator and returns them.\n    operators_50 = list(itertools.islice(operator_with_random_window(images, domain),\n                                    50))\n\n    comb = CombinationPattern(*operators_50)\n    Z, y = comb.extract_dataset(images2, True)\n    # nilc_precomputed requires the target vector coded in 0-1 labels\n    y = y / 255\n\n    which_ops, cls2nd, progress = nilc_precomputed(Z, y, lamb=4.2, max_age=10)\n\n    selected_transforms = [op for i, op in enumerate(operators_50) if i in which_ops]\n    comb_final = CombinationPattern(*selected_transforms)\n\n    op2 = trios.WOperator(domain, cls2nd, comb_final)\n\n    print('Final error:', op2.eval(test))\n    print('Number of transforms:', len(op2.extractor))\n    plot_progress(progress)\n\n\n\n\n\nFinal error: 0.0043036171599157465\nNumber of transforms: 8\n\n\n\n\nUsing NILC pre-computed requires some setup. First, it is necessary to train\nall \nmax_iter\n operators and apply them to the training images. Use use the\nsame \noperator_with_random_window\n function to generate operators to ensure that\nboth versions will yield the same results. Second, we must apply all operators\nto a second training set (\nimages2\n). We do this by exploting the \nCombinationPattern\n\nextractor (described in \nTwo-level\n transforms). \nnilc_precomputed\n\nalso expects the target vector \ny\n to be coded using \n0\n or \n1\n labels, so we \nneed to convert as well. \n\n\nAfter \nnilc_precomputed\n we need to assemble a two level transform, since it \nonly receives the output of the combined transforms but not the transforms \nthemselves.\n\n\n\n\nBoth versions of NILC produce the same output, but each is more adequate for \ncertain situations. Also notice that the results are superior to simples \napproaches. The only necessary extra step was to calibrate \\(\\lambda\\) so that\nthe combination has a reasonable number of transforms.",
            "title": "NILC"
        },
        {
            "location": "/methods/nilc/#nilc",
            "text": "Reference: I. S. Montagner, N. S. T. Hirata, R. Hirata and S. Canu, \"NILC: A \ntwo level learning algorithm with operator selection,\" 2016 IEEE International \nConference on Image Processing (ICIP), Phoenix, AZ, 2016, pp. 1873-1877.    NILC (short for Near Infinite Linear Combination) is a method for automatically \ncreating combinations of image transforms, also known as  two \nlevel  transforms.   NILC receives as input a large neighborhood window  W  and a regularization \nparameter \\(\\lambda\\). It works builds a combination of transformations by, \nat each iteration, selecting a random image transform (based on a random subset \nof window  W ) and then verifying if it can improve the current combination. In \nthe positive case it includes this new transform (and possibly discards others).\nThis process is repeat when no more improvements are found after  max_age  iterations\nor after a maximum of  max_iter . The \\(\\lambda\\) parameter controls how many\ntransforms are selected and is typicall larger than \\(1\\).   There are two ways of using NILC. The first way ( on the fly ) executes the \nmethod iteravely and runs for at most  max_iter  iterations, but possibly less.\nThe second method ( pre-computed ) first trains all  max_iter  transforms and \nthen executes the NILC algorithm. Although there is large possibility of wasting\nresources training transforms that will not be used, this allows faster iteration\nwhen determining \\(\\lambda\\).",
            "title": "NILC"
        },
        {
            "location": "/methods/nilc/#on-the-fly",
            "text": "The code below illustrates how to use NILC  on the fly . This is good if you happen\nto know a very good value for \\(\\lambda\\) or if it can be easily determined somehow.  # file docs/examples/methods/nilc1.py\nimport trios\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.classifiers import SKClassifier\nfrom trios.feature_extractors import RAWFeatureExtractor\n\nfrom trios.contrib.nilc.nilc import nilc, plot_progress\nimport trios.shortcuts.window as w\n\nimport numpy as np\n\n# Activate logging so that we can follow NILC progress\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\ndef operator_with_random_window(training_set, domain_window):\n    rnd = np.random.RandomState(12) # set this to select the same windows everytime\n    while True:\n        win = w.random_win(domain_window, 40, rndstate=rnd)\n        op = trios.WOperator(win, \n                             SKClassifier(DecisionTreeClassifier()), \n                             RAWFeatureExtractor)\n        op.train(training_set)\n        yield op\n\nif __name__ == '__main__':\n    np.random.seed(10) \n    images = trios.Imageset.read('../jung-images/level1.set')\n    images2 = trios.Imageset.read('../jung-images/level2.set')\n    test = trios.Imageset.read('../jung-images/test.set')\n\n    domain = np.ones((9, 7), np.uint8)\n\n    op2, progress = nilc(images, images2, operator_with_random_window, domain, lamb=4.2, max_iter=50, max_age=10)\n\n    print('Final error:', op2.eval(test))\n    print('Number of transforms:', len(op2.extractor))\n    plot_progress(progress)  \nFinal error: 0.0043036171599157465\nNumber of transforms: 8  On the fly NILC receives many arguments. The first two are  trios.Imageset s to \nbe used during training.   The  operator_with_random_window  function can either returning a list of  max_iter  operators or it can be a generator that yields new operators. If you \ndo not know what a generator function is follow  this \nlink  for a simple \nexplanation. Or just follow the template above and create a function with an \ninfinite loop and a  yield  inside it.   The  domain  argument is a large neighborhood window. All transforms combined\nare restricted to this window. The  lamb ,  max_iter  and  max_age  arguments\nwere already explained above.",
            "title": "On the fly"
        },
        {
            "location": "/methods/nilc/#pre-computed",
            "text": "The code below illustrates how to use NILC  precomputed . This is useful if you\ndo not know which \\(\\lambda\\) to use and want to run the method many times with\ndifferent values.  # file docs/examples/methods/nilc2.py\nimport trios\nfrom trios.contrib.nilc.nilc import nilc_precomputed, plot_progress\nimport trios.shortcuts.window as w\n\nimport numpy as np\n\n# Activate logging so that we can follow NILC progress\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\nfrom nilc1 import operator_with_random_window\nfrom trios.feature_extractors import CombinationPattern\nimport itertools\n\nif __name__ == '__main__':\n    np.random.seed(10) \n    images = trios.Imageset.read('../jung-images/level1.set')\n    images2 = trios.Imageset.read('../jung-images/level2.set')\n    test = trios.Imageset.read('../jung-images/test.set')\n\n    domain = np.ones((9, 7), np.uint8)\n\n    # Train all 50 transforms before starting NILC and apply them to images2\n    # This takes the fist 50 elements from the iterator and returns them.\n    operators_50 = list(itertools.islice(operator_with_random_window(images, domain),\n                                    50))\n\n    comb = CombinationPattern(*operators_50)\n    Z, y = comb.extract_dataset(images2, True)\n    # nilc_precomputed requires the target vector coded in 0-1 labels\n    y = y / 255\n\n    which_ops, cls2nd, progress = nilc_precomputed(Z, y, lamb=4.2, max_age=10)\n\n    selected_transforms = [op for i, op in enumerate(operators_50) if i in which_ops]\n    comb_final = CombinationPattern(*selected_transforms)\n\n    op2 = trios.WOperator(domain, cls2nd, comb_final)\n\n    print('Final error:', op2.eval(test))\n    print('Number of transforms:', len(op2.extractor))\n    plot_progress(progress)  \nFinal error: 0.0043036171599157465\nNumber of transforms: 8  Using NILC pre-computed requires some setup. First, it is necessary to train\nall  max_iter  operators and apply them to the training images. Use use the\nsame  operator_with_random_window  function to generate operators to ensure that\nboth versions will yield the same results. Second, we must apply all operators\nto a second training set ( images2 ). We do this by exploting the  CombinationPattern \nextractor (described in  Two-level  transforms).  nilc_precomputed \nalso expects the target vector  y  to be coded using  0  or  1  labels, so we \nneed to convert as well.   After  nilc_precomputed  we need to assemble a two level transform, since it \nonly receives the output of the combined transforms but not the transforms \nthemselves.   Both versions of NILC produce the same output, but each is more adequate for \ncertain situations. Also notice that the results are superior to simples \napproaches. The only necessary extra step was to calibrate \\(\\lambda\\) so that\nthe combination has a reasonable number of transforms.",
            "title": "Pre-computed"
        },
        {
            "location": "/methods/ka/",
            "text": "Kernel Approximations - KA\n\n\n\n\nReference: I. S. Montagner, R. Hirata, N. S. T. Hirata and S. Canu, \"Kernel Approximations for W-Operator Learning,\" 2016 29th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), Sao Paulo, 2016, pp. 386-393.\n\n\n\n\nKA is a method that employs \nkernels\n to process image. Think of a kernel as a similarity function between two images. It they are similar its value is close to \n1\n, but if they are different it is closer to \n0\n. There are two kernels in TRIOS: \nGaussian\n (adequate for gray-level images) and \nPolynomial\n (adequate for binary images). KA works as a feature extractor such that the inner product between two computed feature vectors (two image patches) represents their similarity. \n\n\nAlthough KA can be used with any classifier, best results are achieved using \nSVMs\n. Since this classifier may require a large amount of memory, we only use a subset of the training images for training (otherwise memory requirements blow up). In both examples we use scikit-learn's implementation of linear SVMs and a fixed subset of \n20.000\n training samples. \n\n\nBinary images and the polynomial kernel\n\n\nTo use a polynomial kernel we must determine the degree of the polynomial used. In all experiments in the reference article found \n3\n to be the best value for this parameter. We also need to determine the number of computed features. We fix this parameter as \n1000\n in this example. The larger the number of computed features the larger the memory consumption will be.\n\n\n# file docs/examples/methods/ka1.py\nfrom trios.contrib.kern_approx import NystromFeatures\nfrom trios.classifiers import SKClassifier\nfrom trios.feature_extractors import RAWFeatureExtractor\nimport trios\nimport numpy as np\n\nfrom sklearn.svm import LinearSVC\n\nif __name__ == '__main__':\n    np.random.seed(10) \n    images = trios.Imageset.read('../jung-images/training.set')\n    test = trios.Imageset.read('../jung-images/test.set')\n\n    domain = np.ones((9, 7), np.uint8)\n    raw = RAWFeatureExtractor(domain)\n    ka_features = NystromFeatures(raw, images, n_components=1000, \n        kernel='poly', degree=3, batch_size=20000)\n    svm = SKClassifier(LinearSVC(), partial=True)\n    op = trios.WOperator(domain, svm, ka_features)\n    op.train(images)\n\n    print('Error:', op.eval(test))\n\n\n\n\n\nError: 0.007091876164931582\n\n\n\n\nGray level images and the gaussian kernel\n\n\nTo use a gaussian kernel we must determine its bandwidth \\(\\gamma\\), which is typicall a small value between \n0.1\n and \n0.00001\n. In the article above we have determined \\(\\gamma=0.01\\) as a good value for the DRIVE dataset and use \n200\n computed features. \n\n\n\n\nThis examples needs the DRIVE dataset (\ndownload\n)\n\n\n\n\n# file docs/examples/methods/ka2.py\nfrom trios.contrib.kern_approx import NystromFeatures\nfrom trios.classifiers import SKClassifier\nfrom trios.feature_extractors import Aperture\nimport trios\nimport numpy as np\n\nfrom sklearn.svm import LinearSVC\n\nfrom aperture import training, testset\n\nif __name__ == '__main__':\n    np.random.seed(10) \n    domain = np.ones((11, 11), np.uint8)\n    ap = Aperture(domain, k=10, mul=0.5)\n    ka_features = NystromFeatures(ap, training, n_components=200, \n        kernel='gaussian', gamma=0.01, batch_size=20000)\n    svm = SKClassifier(LinearSVC(), partial=True)\n\n    op = trios.WOperator(domain, svm, ka_features)\n    op.train(training)\n\n    print('Accuracy:', 1 - op.eval(testset[:2]))\n\n\n\n\n\n\n\nAccuracy: 0.934125981168681\n\n\n\n\nThis value is much better than the one we obtained using \nAperture\n\nalone! By using larger computed feature vectors and larger training sets we can\nimprove even more this result.",
            "title": "KA"
        },
        {
            "location": "/methods/ka/#kernel-approximations-ka",
            "text": "Reference: I. S. Montagner, R. Hirata, N. S. T. Hirata and S. Canu, \"Kernel Approximations for W-Operator Learning,\" 2016 29th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), Sao Paulo, 2016, pp. 386-393.   KA is a method that employs  kernels  to process image. Think of a kernel as a similarity function between two images. It they are similar its value is close to  1 , but if they are different it is closer to  0 . There are two kernels in TRIOS:  Gaussian  (adequate for gray-level images) and  Polynomial  (adequate for binary images). KA works as a feature extractor such that the inner product between two computed feature vectors (two image patches) represents their similarity.   Although KA can be used with any classifier, best results are achieved using  SVMs . Since this classifier may require a large amount of memory, we only use a subset of the training images for training (otherwise memory requirements blow up). In both examples we use scikit-learn's implementation of linear SVMs and a fixed subset of  20.000  training samples.",
            "title": "Kernel Approximations - KA"
        },
        {
            "location": "/methods/ka/#binary-images-and-the-polynomial-kernel",
            "text": "To use a polynomial kernel we must determine the degree of the polynomial used. In all experiments in the reference article found  3  to be the best value for this parameter. We also need to determine the number of computed features. We fix this parameter as  1000  in this example. The larger the number of computed features the larger the memory consumption will be.  # file docs/examples/methods/ka1.py\nfrom trios.contrib.kern_approx import NystromFeatures\nfrom trios.classifiers import SKClassifier\nfrom trios.feature_extractors import RAWFeatureExtractor\nimport trios\nimport numpy as np\n\nfrom sklearn.svm import LinearSVC\n\nif __name__ == '__main__':\n    np.random.seed(10) \n    images = trios.Imageset.read('../jung-images/training.set')\n    test = trios.Imageset.read('../jung-images/test.set')\n\n    domain = np.ones((9, 7), np.uint8)\n    raw = RAWFeatureExtractor(domain)\n    ka_features = NystromFeatures(raw, images, n_components=1000, \n        kernel='poly', degree=3, batch_size=20000)\n    svm = SKClassifier(LinearSVC(), partial=True)\n    op = trios.WOperator(domain, svm, ka_features)\n    op.train(images)\n\n    print('Error:', op.eval(test))  \nError: 0.007091876164931582",
            "title": "Binary images and the polynomial kernel"
        },
        {
            "location": "/methods/ka/#gray-level-images-and-the-gaussian-kernel",
            "text": "To use a gaussian kernel we must determine its bandwidth \\(\\gamma\\), which is typicall a small value between  0.1  and  0.00001 . In the article above we have determined \\(\\gamma=0.01\\) as a good value for the DRIVE dataset and use  200  computed features.    This examples needs the DRIVE dataset ( download )   # file docs/examples/methods/ka2.py\nfrom trios.contrib.kern_approx import NystromFeatures\nfrom trios.classifiers import SKClassifier\nfrom trios.feature_extractors import Aperture\nimport trios\nimport numpy as np\n\nfrom sklearn.svm import LinearSVC\n\nfrom aperture import training, testset\n\nif __name__ == '__main__':\n    np.random.seed(10) \n    domain = np.ones((11, 11), np.uint8)\n    ap = Aperture(domain, k=10, mul=0.5)\n    ka_features = NystromFeatures(ap, training, n_components=200, \n        kernel='gaussian', gamma=0.01, batch_size=20000)\n    svm = SKClassifier(LinearSVC(), partial=True)\n\n    op = trios.WOperator(domain, svm, ka_features)\n    op.train(training)\n\n    print('Accuracy:', 1 - op.eval(testset[:2]))  \nAccuracy: 0.934125981168681  This value is much better than the one we obtained using  Aperture \nalone! By using larger computed feature vectors and larger training sets we can\nimprove even more this result.",
            "title": "Gray level images and the gaussian kernel"
        },
        {
            "location": "/methods/lbp/",
            "text": "Local Binary Pattern\n\n\n\n\nReference: T. Ojala, et al. \"Performance evaluation of texture measures with classification based on Kullback discrimination of distributions\", ICPR 1994, vol. 1, pp. 582 - 585.\n\n\n\n\nLocal Binary Pattern is a texture descriptor, introduced by Ojala et. al in 1994, based on the idea that a\ntexture can be described by two measures: local spatial pattern and grayscale contrast. Each pixel on the \nimage is labeled by thresholding the surrounding pixels with the value of the central pixel. Then, the resulting \nneighborhood is transformed into a binary number that is used as the label of the central pixel. TRIOSlib \nimplementation of LBP uses the texture descriptor algorithm as a feature extractor.\n\n\nThe code below shows an example of learning an image operator from the \nCharS\n dataset using LBP as its feature\nextractor. Download the CharS dataset and modify the \nchar_location\n variable to the directory where the dataset\nwas extracted.\n\n\n# file docs/examples/methods/lbp.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.contrib.features.lbp import LBPExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\nchars_location = 'datasets/'\ntraining = trios.Imageset.read(chars_location + 'characters/training.set')\ntestset = trios.Imageset.read(chars_location + 'characters/test.set')\n\nif __name__ == \"__main__\":\n    win = np.ones((9,9), np.uint8)\n    op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), LBPExtractor)\n    print('Training')\n    op.train(training)\n    print('Evaluating')\n    print('Accuracy:', 1 - op.eval(testset))\n\n\n\n\n\nAccuracy: 0.9896342171863871",
            "title": "LBP"
        },
        {
            "location": "/methods/lbp/#local-binary-pattern",
            "text": "Reference: T. Ojala, et al. \"Performance evaluation of texture measures with classification based on Kullback discrimination of distributions\", ICPR 1994, vol. 1, pp. 582 - 585.   Local Binary Pattern is a texture descriptor, introduced by Ojala et. al in 1994, based on the idea that a\ntexture can be described by two measures: local spatial pattern and grayscale contrast. Each pixel on the \nimage is labeled by thresholding the surrounding pixels with the value of the central pixel. Then, the resulting \nneighborhood is transformed into a binary number that is used as the label of the central pixel. TRIOSlib \nimplementation of LBP uses the texture descriptor algorithm as a feature extractor.  The code below shows an example of learning an image operator from the  CharS  dataset using LBP as its feature\nextractor. Download the CharS dataset and modify the  char_location  variable to the directory where the dataset\nwas extracted.  # file docs/examples/methods/lbp.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.contrib.features.lbp import LBPExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\nchars_location = 'datasets/'\ntraining = trios.Imageset.read(chars_location + 'characters/training.set')\ntestset = trios.Imageset.read(chars_location + 'characters/test.set')\n\nif __name__ == \"__main__\":\n    win = np.ones((9,9), np.uint8)\n    op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), LBPExtractor)\n    print('Training')\n    op.train(training)\n    print('Evaluating')\n    print('Accuracy:', 1 - op.eval(testset))  \nAccuracy: 0.9896342171863871",
            "title": "Local Binary Pattern"
        },
        {
            "location": "/methods/moments/",
            "text": "Moments\n\n\n\n\nReference: M. K. Hu, \"Visual Pattern Recognition by Moment Invariants\", IRE Trans. Info. Theory, vol. IT-8, pp.179\u2013187, 1962.\n\n\n\n\nImage moments are a set of particular weighted averages of the intensity values of the pixels of the image. These\nvalues can be used to calculate some properties of the image, such as area or centroid value. The nth-order \nmoment can be calculated by the following equation:\n\n\n()\n\n\nAlthough many improvements and modifications on image moments were proposed throughout the years, \nsuch as the Zernike Moments, TRIOSlib implementation is based on Hu moments, without any further modification or\nimprovement. \n\n\nMoments is a feature extractor implemented on TRIOSlib. It receives a parameter \norder\n that indicates the\nmaximum order to be calculated. The feature extractor, then, calculates the nth-order moments for every\nn that is less or equal than the given parameter. If no parameter is given, the feature extractor defaults\nto \norder = 2\n.\n\n\n# file docs/examples/methods/moments.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.contrib.features.moments import MomentsExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\ndrive_location = 'datasets/'\ntraining = trios.Imageset([\n    ('%s/training/images/%2d_training.tif'%(drive_location, i),\n    '%s/training/1st_manual/%2d_manual1.gif'%(drive_location, i),\n    '%s/training/mask/%2d_training_mask.gif'%(drive_location, i))\n    for i in range(21, 41)])\n\ntestset = trios.Imageset([\n    ('%s/test/images/%02d_test.tif'%(drive_location, i),\n    '%s/test/1st_manual/%02d_manual1.gif'%(drive_location, i),\n    '%s/test/mask/%02d_test_mask.gif'%(drive_location, i))\n    for i in range(1, 21)])\n\nif __name__ == '__main__':\n    win = np.ones((9,9), np.uint8)\n    moments = MomentsExtractor(win, order=4)\n    op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), moments)\n    print('Training')\n    op.train(training)\n    print('Evaluating')\n    print('Accuracy:', 1 - op.eval(testset))\n\n\n\n\n\nAccuracy: 0.7638990221330619",
            "title": "Moments"
        },
        {
            "location": "/methods/moments/#moments",
            "text": "Reference: M. K. Hu, \"Visual Pattern Recognition by Moment Invariants\", IRE Trans. Info. Theory, vol. IT-8, pp.179\u2013187, 1962.   Image moments are a set of particular weighted averages of the intensity values of the pixels of the image. These\nvalues can be used to calculate some properties of the image, such as area or centroid value. The nth-order \nmoment can be calculated by the following equation:  ()  Although many improvements and modifications on image moments were proposed throughout the years, \nsuch as the Zernike Moments, TRIOSlib implementation is based on Hu moments, without any further modification or\nimprovement.   Moments is a feature extractor implemented on TRIOSlib. It receives a parameter  order  that indicates the\nmaximum order to be calculated. The feature extractor, then, calculates the nth-order moments for every\nn that is less or equal than the given parameter. If no parameter is given, the feature extractor defaults\nto  order = 2 .  # file docs/examples/methods/moments.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.contrib.features.moments import MomentsExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\ndrive_location = 'datasets/'\ntraining = trios.Imageset([\n    ('%s/training/images/%2d_training.tif'%(drive_location, i),\n    '%s/training/1st_manual/%2d_manual1.gif'%(drive_location, i),\n    '%s/training/mask/%2d_training_mask.gif'%(drive_location, i))\n    for i in range(21, 41)])\n\ntestset = trios.Imageset([\n    ('%s/test/images/%02d_test.tif'%(drive_location, i),\n    '%s/test/1st_manual/%02d_manual1.gif'%(drive_location, i),\n    '%s/test/mask/%02d_test_mask.gif'%(drive_location, i))\n    for i in range(1, 21)])\n\nif __name__ == '__main__':\n    win = np.ones((9,9), np.uint8)\n    moments = MomentsExtractor(win, order=4)\n    op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), moments)\n    print('Training')\n    op.train(training)\n    print('Evaluating')\n    print('Accuracy:', 1 - op.eval(testset))  \nAccuracy: 0.7638990221330619",
            "title": "Moments"
        },
        {
            "location": "/methods/fourier/",
            "text": "Fourier\n\n\nFourier Transform is a transformation that decomposes a function of time into its frequencies, in other words,\nit transform a function in the time domain to its representation on the frequency domain. This transformation\nis very used in signal analysis and processing, and it can also be used in image processing. \n\n\nTRIOSlib implements a feature extractor based on Fourier Transform. This feature extractor receives a region\nof the image and applies the Fourier Transform in those pixels. Then, the coefficients of the region in the\nfrequency domain are used as features.\n\n\nThis feature performed the best while separating text and images. While the performance was similar to using\nthe raw pixel intensities values, the dimension of the resulting feature vector was considerably smaller when\nusing the fourier feature.\n\n\n# file docs/examples/methods/fourier.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.contrib.features.fourier import FourierExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\ndrive_location = 'datasets/drive'\ntraining = trios.Imageset([\n    ('%s/training/images/%2d_training.tif'%(drive_location, i),\n    '%s/training/1st_manual/%2d_manual1.gif'%(drive_location, i),\n    '%s/training/mask/%2d_training_mask.gif'%(drive_location, i))\n    for i in range(21, 41)])\n\ntestset = trios.Imageset([\n    ('%s/test/images/%02d_test.tif'%(drive_location, i),\n    '%s/test/1st_manual/%02d_manual1.gif'%(drive_location, i),\n    '%s/test/mask/%02d_test_mask.gif'%(drive_location, i))\n    for i in range(1, 21)])\n\nif __name__ == '__main__':\n   win = np.ones((9,9), np.uint8)\n   op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), FourierExtractor)\n   print('Training')\n   op.train(training)\n   print('Evaluating')\n   print('Accuracy:', 1 - op.eval(testset))\n\n\n\n\n\nAccuracy: 0.8501032250416084",
            "title": "Fourier"
        },
        {
            "location": "/methods/fourier/#fourier",
            "text": "Fourier Transform is a transformation that decomposes a function of time into its frequencies, in other words,\nit transform a function in the time domain to its representation on the frequency domain. This transformation\nis very used in signal analysis and processing, and it can also be used in image processing.   TRIOSlib implements a feature extractor based on Fourier Transform. This feature extractor receives a region\nof the image and applies the Fourier Transform in those pixels. Then, the coefficients of the region in the\nfrequency domain are used as features.  This feature performed the best while separating text and images. While the performance was similar to using\nthe raw pixel intensities values, the dimension of the resulting feature vector was considerably smaller when\nusing the fourier feature.  # file docs/examples/methods/fourier.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.contrib.features.fourier import FourierExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\ndrive_location = 'datasets/drive'\ntraining = trios.Imageset([\n    ('%s/training/images/%2d_training.tif'%(drive_location, i),\n    '%s/training/1st_manual/%2d_manual1.gif'%(drive_location, i),\n    '%s/training/mask/%2d_training_mask.gif'%(drive_location, i))\n    for i in range(21, 41)])\n\ntestset = trios.Imageset([\n    ('%s/test/images/%02d_test.tif'%(drive_location, i),\n    '%s/test/1st_manual/%02d_manual1.gif'%(drive_location, i),\n    '%s/test/mask/%02d_test_mask.gif'%(drive_location, i))\n    for i in range(1, 21)])\n\nif __name__ == '__main__':\n   win = np.ones((9,9), np.uint8)\n   op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), FourierExtractor)\n   print('Training')\n   op.train(training)\n   print('Evaluating')\n   print('Accuracy:', 1 - op.eval(testset))  \nAccuracy: 0.8501032250416084",
            "title": "Fourier"
        },
        {
            "location": "/methods/sobel/",
            "text": "Sobel\n\n\n\n\nReference: Sobel, Irwin. (2014). An Isotropic 3 3 Image Gradient Operator. Presentation at Stanford A.I. Project 1968\n\n\n\n\nSobel Operator is used in image processing to create an image that emphasizes the edges of the original image.\nThis operator uses two different 3x3 kernels that are convolved with the original image, creating two distinct\nimages, where one emphasize the horizontal edges and the other emphasize the vertical ones. With these two \nimages, the gradient's magnitude and the gradient's direction can be calculated.\n\n\nThe feature extractor based on the Sobel operator convolve the original image (limited by the given mask) with\nthe two Sobel kerneles. Then, it calculates the edge magnitude of the region of interest, and returns the\nedge magnitude as a feature vector.\n\n\n# file docs/examples/methods/sobel.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.contrib.features.sobel import SobelExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\ndrive_location = 'datasets/drive'\ntraining = trios.Imageset([\n    ('%s/training/images/%2d_training.tif'%(drive_location, i),\n    '%s/training/1st_manual/%2d_manual1.gif'%(drive_location, i),\n    '%s/training/mask/%2d_training_mask.gif'%(drive_location, i))\n    for i in range(21, 41)])\n\ntestset = trios.Imageset([\n    ('%s/test/images/%02d_test.tif'%(drive_location, i),\n    '%s/test/1st_manual/%02d_manual1.gif'%(drive_location, i),\n    '%s/test/mask/%02d_test_mask.gif'%(drive_location, i))\n    for i in range(1, 21)])\n\nif __name__ == '__main__':\n    win = np.ones((9,9), np.uint8)\n    sobel = SobelExtractor(window=win)\n    op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), sobel)\n    print('Training')\n    op.train(training)\n    print('Evaluating')\n    print('Accuracy:', 1 - op.eval(testset))",
            "title": "Sobel"
        },
        {
            "location": "/methods/sobel/#sobel",
            "text": "Reference: Sobel, Irwin. (2014). An Isotropic 3 3 Image Gradient Operator. Presentation at Stanford A.I. Project 1968   Sobel Operator is used in image processing to create an image that emphasizes the edges of the original image.\nThis operator uses two different 3x3 kernels that are convolved with the original image, creating two distinct\nimages, where one emphasize the horizontal edges and the other emphasize the vertical ones. With these two \nimages, the gradient's magnitude and the gradient's direction can be calculated.  The feature extractor based on the Sobel operator convolve the original image (limited by the given mask) with\nthe two Sobel kerneles. Then, it calculates the edge magnitude of the region of interest, and returns the\nedge magnitude as a feature vector.  # file docs/examples/methods/sobel.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.contrib.features.sobel import SobelExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\ndrive_location = 'datasets/drive'\ntraining = trios.Imageset([\n    ('%s/training/images/%2d_training.tif'%(drive_location, i),\n    '%s/training/1st_manual/%2d_manual1.gif'%(drive_location, i),\n    '%s/training/mask/%2d_training_mask.gif'%(drive_location, i))\n    for i in range(21, 41)])\n\ntestset = trios.Imageset([\n    ('%s/test/images/%02d_test.tif'%(drive_location, i),\n    '%s/test/1st_manual/%02d_manual1.gif'%(drive_location, i),\n    '%s/test/mask/%02d_test_mask.gif'%(drive_location, i))\n    for i in range(1, 21)])\n\nif __name__ == '__main__':\n    win = np.ones((9,9), np.uint8)\n    sobel = SobelExtractor(window=win)\n    op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), sobel)\n    print('Training')\n    op.train(training)\n    print('Evaluating')\n    print('Accuracy:', 1 - op.eval(testset))",
            "title": "Sobel"
        },
        {
            "location": "/methods/hog/",
            "text": "Histogram of Oriented Gradients\n\n\n\n\nReference: Dalal, Navneet, and Bill Triggs. \"Histograms of oriented gradients for human detection.\" Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. Vol. 1. IEEE, 2005.\n\n\n\n\nHistogram of Oriented Gradients is a feature descriptor mainly used for object detection. It's similar to \nscale-invariant features descriptors, such as SIFT or SURF. This technique counts gradient orientation ocurrences\nin uniformly spaced cells, using overlapping local contrast to improve accuracy. The original technique divides\nthe image in cells and calculates the histogram of oriented gradients in each cell. The feature extractor\nimplemented in TRIOSlib only calculates the histogram of oriented gradients in the region of interest and uses\nthis histogram as a feature.\n\n\nHOG feature extractor takes two additional and optional arguments, \nchannels\n and \nnormalize\n. The \nchannels\n\nargument controls how many orientation bins the histogram will have, dividing the range of orientation in uniform\nintervals. The value of each orientation bin can be normalized, and this is controled by the \nnormalize\n \nargument.\n\n\n# file docs/examples/methods/hog.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.contrib.features.hog import HoGExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\ndrive_location = 'datasets/drive'\ntraining = trios.Imageset([\n    ('%s/training/images/%2d_training.tif'%(drive_location, i),\n    '%s/training/1st_manual/%2d_manual1.gif'%(drive_location, i),\n    '%s/training/mask/%2d_training_mask.gif'%(drive_location, i))\n    for i in range(21, 41)])\n\ntestset = trios.Imageset([\n    ('%s/test/images/%02d_test.tif'%(drive_location, i),\n    '%s/test/1st_manual/%02d_manual1.gif'%(drive_location, i),\n    '%s/test/mask/%02d_test_mask.gif'%(drive_location, i))\n    for i in range(1, 21)])\n\nif __name__ == '__main__':\n    win = np.ones((9,9), np.uint8)\n    hog = HoGExtractor(window=win, channels=10, normalize=True)\n\n    op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), hog)\n    print('Training')\n    op.train(training)\n    print('Evaluating')\n    print('Accuracy:', 1 - op.eval(testset))",
            "title": "HoG"
        },
        {
            "location": "/methods/hog/#histogram-of-oriented-gradients",
            "text": "Reference: Dalal, Navneet, and Bill Triggs. \"Histograms of oriented gradients for human detection.\" Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. Vol. 1. IEEE, 2005.   Histogram of Oriented Gradients is a feature descriptor mainly used for object detection. It's similar to \nscale-invariant features descriptors, such as SIFT or SURF. This technique counts gradient orientation ocurrences\nin uniformly spaced cells, using overlapping local contrast to improve accuracy. The original technique divides\nthe image in cells and calculates the histogram of oriented gradients in each cell. The feature extractor\nimplemented in TRIOSlib only calculates the histogram of oriented gradients in the region of interest and uses\nthis histogram as a feature.  HOG feature extractor takes two additional and optional arguments,  channels  and  normalize . The  channels \nargument controls how many orientation bins the histogram will have, dividing the range of orientation in uniform\nintervals. The value of each orientation bin can be normalized, and this is controled by the  normalize  \nargument.  # file docs/examples/methods/hog.py\nfrom trios.classifiers import SKClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom trios.contrib.features.hog import HoGExtractor\nimport trios\nimport numpy as np\n\nimport trios.shortcuts.persistence as p\n\ndrive_location = 'datasets/drive'\ntraining = trios.Imageset([\n    ('%s/training/images/%2d_training.tif'%(drive_location, i),\n    '%s/training/1st_manual/%2d_manual1.gif'%(drive_location, i),\n    '%s/training/mask/%2d_training_mask.gif'%(drive_location, i))\n    for i in range(21, 41)])\n\ntestset = trios.Imageset([\n    ('%s/test/images/%02d_test.tif'%(drive_location, i),\n    '%s/test/1st_manual/%02d_manual1.gif'%(drive_location, i),\n    '%s/test/mask/%02d_test_mask.gif'%(drive_location, i))\n    for i in range(1, 21)])\n\nif __name__ == '__main__':\n    win = np.ones((9,9), np.uint8)\n    hog = HoGExtractor(window=win, channels=10, normalize=True)\n\n    op = trios.WOperator(win, SKClassifier(DecisionTreeClassifier()), hog)\n    print('Training')\n    op.train(training)\n    print('Evaluating')\n    print('Accuracy:', 1 - op.eval(testset))",
            "title": "Histogram of Oriented Gradients"
        },
        {
            "location": "/publications-using-trioslib/",
            "text": "Publications using TRIOSlib\n\n\nIf your paper is not cited here, please contact the project maintainers.\n\n\n\n\nJulca-Aguilar, Frank D., and Nina ST Hirata. \"Image operator learning coupled with CNN classification and its application to staff line removal.\" arXiv preprint arXiv:1709.06476 (2017).\n\n\nMontagner, Igor S., Nina ST Hirata, and Roberto Hirata. \"Staff removal using image operator learning.\" Pattern Recognition 63 (2017): 310-320.\n\n\nMontagner, Igor S., Nina ST Hirata, and Roberto Hirata. \"Image operator learning and applications.\" Graphics, Patterns and Images Tutorials (SIBGRAPI-T), SIBGRAPI Conference on. IEEE, 2016.\n\n\nMontagner, I. S.; Canu, S.; Hirata, N.S.T.; Hirata Jr., R.; \"Kernel approximations for $W$-operator learning\", Conference on Graphics, Patterns and Images (SIBGRAPI), 2016;\n\n\nMontagner, I. S.; Hirata, N.S.T.; Hirata Jr., R.; Canu, S.; \"NILC: a two level learning algorithm with operator selection\", IEEE International Conference on Image Processing (ICIP), 2016;\n\n\nHirata, Nina ST, Igor S. Montagner, and Roberto Hirata Jr. \"Comics image processing: learning to segment text.\" Proceedings of the 1st International Workshop on coMics ANalysis, Processing and Understanding. ACM, 2016.\n\n\nDornelles, M.M; Hirata, N. S. T.; \"Selection of windows for W-operator combination from entropy based ranking\",  Conference on Graphics, Patterns and Images (SIBGRAPI), 2015;\n\n\nMontagner, I. S.; Hirata, R.; Hirata, N.S.T., \"Learning to remove staff lines from music score images,\" IEEE International Conference on Image Processing (ICIP), 2014;\n\n\nMontagner, I. S.; Hirata, R.; Hirata, N.S.T., \"A Machine Learning Based Method for Staff Removal,\" 22nd International Conference on Pattern Recognition (ICPR), 2014;\n\n\nMontagner, I. S.; Hirata, R.; Hirata, N.S.T., \"TRIOS - an open source toolbox for training image operators from samples\". Workshop of Works in Progress (WIP) in SIBGRAPI 2012 (XXV Conference on Graphics, Patterns and Images);",
            "title": "Publications"
        },
        {
            "location": "/publications-using-trioslib/#publications-using-trioslib",
            "text": "If your paper is not cited here, please contact the project maintainers.   Julca-Aguilar, Frank D., and Nina ST Hirata. \"Image operator learning coupled with CNN classification and its application to staff line removal.\" arXiv preprint arXiv:1709.06476 (2017).  Montagner, Igor S., Nina ST Hirata, and Roberto Hirata. \"Staff removal using image operator learning.\" Pattern Recognition 63 (2017): 310-320.  Montagner, Igor S., Nina ST Hirata, and Roberto Hirata. \"Image operator learning and applications.\" Graphics, Patterns and Images Tutorials (SIBGRAPI-T), SIBGRAPI Conference on. IEEE, 2016.  Montagner, I. S.; Canu, S.; Hirata, N.S.T.; Hirata Jr., R.; \"Kernel approximations for $W$-operator learning\", Conference on Graphics, Patterns and Images (SIBGRAPI), 2016;  Montagner, I. S.; Hirata, N.S.T.; Hirata Jr., R.; Canu, S.; \"NILC: a two level learning algorithm with operator selection\", IEEE International Conference on Image Processing (ICIP), 2016;  Hirata, Nina ST, Igor S. Montagner, and Roberto Hirata Jr. \"Comics image processing: learning to segment text.\" Proceedings of the 1st International Workshop on coMics ANalysis, Processing and Understanding. ACM, 2016.  Dornelles, M.M; Hirata, N. S. T.; \"Selection of windows for W-operator combination from entropy based ranking\",  Conference on Graphics, Patterns and Images (SIBGRAPI), 2015;  Montagner, I. S.; Hirata, R.; Hirata, N.S.T., \"Learning to remove staff lines from music score images,\" IEEE International Conference on Image Processing (ICIP), 2014;  Montagner, I. S.; Hirata, R.; Hirata, N.S.T., \"A Machine Learning Based Method for Staff Removal,\" 22nd International Conference on Pattern Recognition (ICPR), 2014;  Montagner, I. S.; Hirata, R.; Hirata, N.S.T., \"TRIOS - an open source toolbox for training image operators from samples\". Workshop of Works in Progress (WIP) in SIBGRAPI 2012 (XXV Conference on Graphics, Patterns and Images);",
            "title": "Publications using TRIOSlib"
        },
        {
            "location": "/examples/",
            "text": "Examples and trained operators\n\n\nTrained Operators\n\n\nThis thesis\n used TRIOS to process several datasets. The image transforms below were used in that work and obtain very good performance. \n\n\n\n\nWhen using these operators (as shown in the \nuser guide\n) for academic purposes do not forget to give credit by citing the papers listed using the \ncite_me()\n method! \n\n\n\n\n\n\njung\n (\ndownload dataset\n) - \ndownload\n, error: 0.004. Uses: [KA][methods/ka.md]\n\n\nveja\n (\ndownload dataset\n) - \ndownload\n, error: 0.017. Uses: \nNILC\n\n\nstaffs\n (\ndownload dataset\n \ndownload\n. Accuracy: 96.71% (white pixels only) . Uses: Uses: \nNILC\n\n\nDRIVE\n (\ndownload dataset\n) - \ndownload\n, accuracy: . Uses: \nKA\n \n\n\n\n\nCode Samples\n\n\nTRIOSlib contains many commented code samples for many common tasks. See the \n\nexamples folder in our repository\n.",
            "title": "Examples and trained operators"
        },
        {
            "location": "/examples/#examples-and-trained-operators",
            "text": "",
            "title": "Examples and trained operators"
        },
        {
            "location": "/examples/#trained-operators",
            "text": "This thesis  used TRIOS to process several datasets. The image transforms below were used in that work and obtain very good performance.    When using these operators (as shown in the  user guide ) for academic purposes do not forget to give credit by citing the papers listed using the  cite_me()  method!     jung  ( download dataset ) -  download , error: 0.004. Uses: [KA][methods/ka.md]  veja  ( download dataset ) -  download , error: 0.017. Uses:  NILC  staffs  ( download dataset   download . Accuracy: 96.71% (white pixels only) . Uses: Uses:  NILC  DRIVE  ( download dataset ) -  download , accuracy: . Uses:  KA",
            "title": "Trained Operators"
        },
        {
            "location": "/examples/#code-samples",
            "text": "TRIOSlib contains many commented code samples for many common tasks. See the  examples folder in our repository .",
            "title": "Code Samples"
        }
    ]
}